{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind prediction - Second assignment\n",
    "\n",
    "## Authors\n",
    "\n",
    "David Moreno Maldonado 100441714     \n",
    "Inés Fernández Campos 100443936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/roni/Desktop/master/2nd quarter/big data intelligence/assignments/assignment_2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import some libraries\n",
    "import os\n",
    "import numpy as np              \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn import preprocessing, impute, model_selection, metrics, neighbors, ensemble, feature_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "import optuna\n",
    "import optuna.visualization as ov\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN PARAMETERS FOR THE ASSIGNMENT\n",
    "budget = 20\n",
    "random_state = 3\n",
    "verbose = 0\n",
    "n_jobs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"wind_pickle\" file contains data in a binary format called \"Pickle\". Pickle data loads faster than text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('wind_pickle.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the attributes in the dataset. Very important, the output attribute (i.e. the value to be predicted, **energy**, is the first attribute). **Steps** represents the hours in advance of the forecast. We will not use this variable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5937, 556)\n"
     ]
    }
   ],
   "source": [
    "# The dataset contains 5937 instances and 556 attributes (including the outcome to be predicted)\n",
    "print(data.shape)\n",
    "#data.columns.values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-1 for training, 0 for validation, 1 for testing\n",
    "year_to_part = {\n",
    "    2005: -1,\n",
    "    2006: -1,\n",
    "    2007: 0,\n",
    "    2008: 0, \n",
    "    2009: 1,\n",
    "    2010: 1\n",
    "}\n",
    "data['partition'] = data['year'].apply(lambda x: year_to_part[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now remove the columns that cannot be used for training the models from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps, month, day, hour, year should be removed, they cannot be used for training the models\n",
    "to_remove = ['steps', 'month', 'year', 'day', 'hour']\n",
    "for m in to_remove: data = data.drop(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets put 163861 missing values \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import randint\n",
    "\n",
    "# we add na values at random\n",
    "my_NIA = 100443936 + 100441714\n",
    "np.random.seed(my_NIA)\n",
    "\n",
    "how_many_nas = round(data.shape[0]*data.shape[1]*0.05)\n",
    "print('Lets put '+str(how_many_nas)+' missing values \\n')\n",
    "x_locations = randint(0, data.shape[0], size=how_many_nas)\n",
    "y_locations = randint(1, data.shape[1]-2, size=how_many_nas)\n",
    "\n",
    "for i in range(len(x_locations)):\n",
    "    data.iat[x_locations[i], y_locations[i]] = np.nan\n",
    "    \n",
    "data.to_pickle('wind_pickle_with_nan.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on, the file wind_pickle_with_nan should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5937, 552)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('wind_pickle_with_nan.pickle')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have randomly inputed missing values throughout our data, prior to creating our models we must impute the missing values (except in the response). In the following cell we have implemented and iterative imputer two different ways (through *knnImputer* and *IterativeImputer*) and finally a simple imputer which is the one we have left uncommented simply because, although the first two are more complex and impute values using the entire set of available feature dimensions to estimate the missing values, they take far too long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().values.any())\n",
    "input_cols = data.columns.difference(['energy', 'partition'])\n",
    "x = data[input_cols]\n",
    "\n",
    "#Iterative imputer (takes too long)\n",
    "'''iter_imp = impute.IterativeImputer(random_state=random_state, \n",
    "                                   initial_strategy='median', \n",
    "                                   max_iter=3,\n",
    "                                   verbose=verbose)\n",
    "no_nan = iter_imp.fit_transform(x)'''\n",
    "\n",
    "#KNN imputer(takes too long)\n",
    "'''knn_imp = impute.KNNImputer(weights='distance')\n",
    "no_nan = knn_imp.fit_transform(x)'''\n",
    "\n",
    "#Simple imputer\n",
    "simp_imp = impute.SimpleImputer(strategy='median',\n",
    "                               verbose=2)\n",
    "no_nan = simp_imp.fit_transform(x)\n",
    "\n",
    "data[input_cols] = pd.DataFrame(data=no_nan)\n",
    "print(data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Lastly we scale the data so that it is all within the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(data[input_cols]) \n",
    "data[input_cols] = scaler.transform(data[input_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "We are going to use train/test for model evaluation (outer) and train/validation for hyperparameter tuning (inner), as follows:     \n",
    "1. Train partition: the first two years of data. Given that there are 6 years worth of data, we will use the first 2/6 of the instances for training.     \n",
    "2. Validation partition: the second two years of data. \n",
    "3. Test partition: the remaining data    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-1 for training, 0 for validation, 1 for testing\n",
    "test = data[data['partition'] == 1]\n",
    "train = data[data['partition'] == -1]\n",
    "val = data[data['partition'] == 0]\n",
    "\n",
    "del test['partition']\n",
    "del train['partition']\n",
    "\n",
    "y_test = test['energy']\n",
    "x_test = test[test.columns.difference(['energy'])]\n",
    "\n",
    "y_train = train['energy']\n",
    "x_train = train[train.columns.difference(['energy'])]\n",
    "\n",
    "\n",
    "y_val = val['energy']\n",
    "x_val = val[train.columns.difference(['energy'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MODEL SELECTION AND HYPER-PARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframes with all the information of each model\n",
    "summary = {\n",
    "    'knn': pd.DataFrame(columns=['Time (sec)', 'Score (RMSE)', 'N. neighbors', 'Weights', 'P']),\n",
    "    'random_forest': pd.DataFrame(columns=['Time (sec)', 'Score (RMSE)', 'Min. samples split', 'Criterion', 'Max. depth', 'N. estimators','Max. features']),\n",
    "    'gradient_boosting': pd.DataFrame(columns=['Time (sec)', 'Score (RMSE)'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Default hyper-parameters\n",
    "\n",
    "Here we train KNN with the default hyper-parameters, so the number of neighbors used will be 5 and the power parameter for the Minkowski metric is set to 2, so KNN will be using euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_state)\n",
    "knn_default = neighbors.KNeighborsRegressor()\n",
    "\n",
    "start_time = time.time()\n",
    "knn_default = knn_default.fit(x_train, y_train)\n",
    "y_val_pred = knn_default.predict(x_val)\n",
    "score = math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "end_time = time.time()\n",
    "\n",
    "summary['knn'] = summary['knn'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': score, \n",
    "    'N. neighbors': 5, \n",
    "    'Weights': 'uniform', \n",
    "    'P': 2\n",
    "    }, \n",
    "    name='default'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Hyper-parameter tunning (OPTUNA)\n",
    "\n",
    "In this subsection however, we are going to tune the hyper-parameters using Optuna.  \n",
    "To do so, we create and objective function that will test a set of hyper-parameters for the model and evaluate the model's performance given those hyper-parameters and return that score. Within this objective function, we define three hyper-parameters to tune: \n",
    "- number of neighbors: taking any integer value in [1, 16].\n",
    "- weights: weight function to be used in prediction can either be uniform weights or points can be distance weighted (closer implies more influence).\n",
    "- p: power parametric for the Minkowski metric in order to choose between euclidean distance or manhattan distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_n_neigbors = 1\n",
    "max_n_neigbors = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-01-14 17:30:52,713]\u001b[0m A new study created in memory with name: no-name-c943cd42-6a4a-416a-8f3b-5afefd078941\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:30:54,935]\u001b[0m Trial 0 finished with value: 427.46488149623633 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'p': 1}. Best is trial 0 with value: 427.46488149623633.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:30:55,027]\u001b[0m Trial 1 finished with value: 435.6329813032633 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 2}. Best is trial 0 with value: 427.46488149623633.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:30:57,211]\u001b[0m Trial 2 finished with value: 441.1599884529427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'p': 1}. Best is trial 0 with value: 427.46488149623633.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:30:57,282]\u001b[0m Trial 3 finished with value: 575.7646217953005 and parameters: {'n_neighbors': 1, 'weights': 'uniform', 'p': 2}. Best is trial 0 with value: 427.46488149623633.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:30:57,377]\u001b[0m Trial 4 finished with value: 433.2464765584705 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'p': 2}. Best is trial 0 with value: 427.46488149623633.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:30:57,474]\u001b[0m Trial 5 finished with value: 434.5528305762055 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'p': 2}. Best is trial 0 with value: 427.46488149623633.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:30:57,573]\u001b[0m Trial 6 finished with value: 439.6983919958271 and parameters: {'n_neighbors': 42, 'weights': 'uniform', 'p': 2}. Best is trial 0 with value: 427.46488149623633.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:30:57,666]\u001b[0m Trial 7 finished with value: 436.9035409437192 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'p': 2}. Best is trial 0 with value: 427.46488149623633.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:30:59,918]\u001b[0m Trial 8 finished with value: 427.434907753109 and parameters: {'n_neighbors': 19, 'weights': 'uniform', 'p': 1}. Best is trial 8 with value: 427.434907753109.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:31:02,098]\u001b[0m Trial 9 finished with value: 429.4748866460311 and parameters: {'n_neighbors': 27, 'weights': 'uniform', 'p': 1}. Best is trial 8 with value: 427.434907753109.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:31:04,291]\u001b[0m Trial 10 finished with value: 430.3611284510964 and parameters: {'n_neighbors': 40, 'weights': 'uniform', 'p': 1}. Best is trial 8 with value: 427.434907753109.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:31:06,484]\u001b[0m Trial 11 finished with value: 427.14402458176176 and parameters: {'n_neighbors': 33, 'weights': 'distance', 'p': 1}. Best is trial 11 with value: 427.14402458176176.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:31:08,682]\u001b[0m Trial 12 finished with value: 429.70298013940806 and parameters: {'n_neighbors': 50, 'weights': 'distance', 'p': 1}. Best is trial 11 with value: 427.14402458176176.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:31:10,926]\u001b[0m Trial 13 finished with value: 429.6001270149181 and parameters: {'n_neighbors': 36, 'weights': 'uniform', 'p': 1}. Best is trial 11 with value: 427.14402458176176.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:31:13,124]\u001b[0m Trial 14 finished with value: 429.5563672247799 and parameters: {'n_neighbors': 34, 'weights': 'uniform', 'p': 1}. Best is trial 11 with value: 427.14402458176176.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:31:15,303]\u001b[0m Trial 15 finished with value: 425.0842222730839 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'p': 1}. Best is trial 15 with value: 425.0842222730839.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:31:17,496]\u001b[0m Trial 16 finished with value: 425.0842222730839 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'p': 1}. Best is trial 15 with value: 425.0842222730839.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:31:19,814]\u001b[0m Trial 17 finished with value: 432.080866275361 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'p': 1}. Best is trial 15 with value: 425.0842222730839.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:31:22,155]\u001b[0m Trial 18 finished with value: 425.0842222730839 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'p': 1}. Best is trial 15 with value: 425.0842222730839.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:31:24,494]\u001b[0m Trial 19 finished with value: 426.6970286295103 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'p': 1}. Best is trial 15 with value: 425.0842222730839.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random_state)\n",
    "def knn_objective(trial):\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', min_n_neigbors, max_n_neigbors)\n",
    "    weights = trial.suggest_categorical('weights', ['uniform','distance'])\n",
    "    p = trial.suggest_categorical('p', [1, 2])\n",
    "\n",
    "    clf = neighbors.KNeighborsRegressor(\n",
    "        n_neighbors=n_neighbors,\n",
    "        weights=weights,\n",
    "        p=p)\n",
    "    \n",
    "    clf = clf.fit(x_train, y_train)\n",
    "    y_val_pred = clf.predict(x_val)\n",
    "    return math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "knn_optuna = optuna.create_study(direction='minimize')\n",
    "start_time = time.time()\n",
    "knn_optuna.optimize(knn_objective, n_trials=budget)\n",
    "end_time = time.time()\n",
    "\n",
    "summary['knn'] = summary['knn'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': knn_optuna.best_value, \n",
    "    'N. neighbors': knn_optuna.best_params['n_neighbors'], \n",
    "    'Weights': knn_optuna.best_params['weights'], \n",
    "    'P': knn_optuna.best_params['p']\n",
    "    }, \n",
    "    name='optuna'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Default hyper-parameters\n",
    "\n",
    "The following cell trains a random forest ensemble with default parameters: 100 estimators, mse criteiron and 2 samples minimum to continue splitting a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_state)\n",
    "rf_default = ensemble.RandomForestRegressor(random_state=random_state, verbose=verbose, n_jobs=n_jobs)\n",
    "\n",
    "start_time = time.time()\n",
    "rf_default = rf_default.fit(x_train, y_train)\n",
    "y_val_pred = rf_default.predict(x_val)\n",
    "score =  math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "end_time = time.time()\n",
    "\n",
    "summary['random_forest'] = summary['random_forest'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': score,\n",
    "    'Min. samples split': 2, \n",
    "    'Criterion': 'mse', \n",
    "    'Max. depth': 'None',\n",
    "    'N. estimators': 100,\n",
    "    'Max. features': 1\n",
    "    },\n",
    "    name='default'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Hyper-parameter tunning (OPTUNA)\n",
    "\n",
    "Once again, instead of settling for the default hyper-parameters we tune them using Optuna. In this case more hyper-parameters are tuned but the procedure is similar if not pratically the same to how we tuned the KNNRegressor using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_depth = 2\n",
    "max_max_depth = 32\n",
    "min_n_estimators = 50\n",
    "max_n_estimators = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-01-14 17:34:47,027]\u001b[0m A new study created in memory with name: no-name-6264c00f-23f0-4876-b70e-50c7b06c51f9\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:34:47,413]\u001b[0m Trial 0 finished with value: 719.4608441722529 and parameters: {'min_samples_split': 0.6575167002537593, 'criterion': 'mae', 'max_depth': 15, 'n_estimators': 204, 'max_features': 0.40181980166836345}. Best is trial 0 with value: 719.4608441722529.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:35:14,502]\u001b[0m Trial 1 finished with value: 399.9725398015795 and parameters: {'min_samples_split': 0.3483686600938284, 'criterion': 'mse', 'max_depth': 21, 'n_estimators': 295, 'max_features': 0.40809651553870596}. Best is trial 1 with value: 399.9725398015795.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:35:14,637]\u001b[0m Trial 2 finished with value: 668.3821156793259 and parameters: {'min_samples_split': 0.7479993441301223, 'criterion': 'mse', 'max_depth': 19, 'n_estimators': 135, 'max_features': 0.5395298588307268}. Best is trial 1 with value: 399.9725398015795.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:49:25,204]\u001b[0m Trial 3 finished with value: 409.6146284572972 and parameters: {'min_samples_split': 0.15665424714233067, 'criterion': 'mae', 'max_depth': 14, 'n_estimators': 197, 'max_features': 0.44182877835747336}. Best is trial 1 with value: 399.9725398015795.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:49:25,522]\u001b[0m Trial 4 finished with value: 719.623890834635 and parameters: {'min_samples_split': 0.6532346689406321, 'criterion': 'mae', 'max_depth': 3, 'n_estimators': 185, 'max_features': 0.10041065470536845}. Best is trial 1 with value: 399.9725398015795.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:49:25,631]\u001b[0m Trial 5 finished with value: 719.7553664741794 and parameters: {'min_samples_split': 0.7080875200226139, 'criterion': 'mae', 'max_depth': 4, 'n_estimators': 56, 'max_features': 0.5457941861737108}. Best is trial 1 with value: 399.9725398015795.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:54:31,946]\u001b[0m Trial 6 finished with value: 428.0612820053403 and parameters: {'min_samples_split': 0.41241325677835017, 'criterion': 'mae', 'max_depth': 24, 'n_estimators': 145, 'max_features': 0.3463888918456644}. Best is trial 1 with value: 399.9725398015795.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:54:48,157]\u001b[0m Trial 7 finished with value: 395.06265119225293 and parameters: {'min_samples_split': 0.19763838215378227, 'criterion': 'mse', 'max_depth': 30, 'n_estimators': 208, 'max_features': 0.24039882694634016}. Best is trial 7 with value: 395.06265119225293.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:54:54,159]\u001b[0m Trial 8 finished with value: 530.7504104958446 and parameters: {'min_samples_split': 0.6125473434735539, 'criterion': 'mse', 'max_depth': 11, 'n_estimators': 195, 'max_features': 0.49601610707025323}. Best is trial 7 with value: 395.06265119225293.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:54:57,157]\u001b[0m Trial 9 finished with value: 562.0889369368881 and parameters: {'min_samples_split': 0.5888334053965973, 'criterion': 'mae', 'max_depth': 30, 'n_estimators': 157, 'max_features': 0.006158945698614859}. Best is trial 7 with value: 395.06265119225293.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:54:57,550]\u001b[0m Trial 10 finished with value: 668.3960024250051 and parameters: {'min_samples_split': 0.976663067631748, 'criterion': 'mse', 'max_depth': 30, 'n_estimators': 372, 'max_features': 0.208449944509364}. Best is trial 7 with value: 395.06265119225293.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:55:24,127]\u001b[0m Trial 11 finished with value: 395.3484020176394 and parameters: {'min_samples_split': 0.16942551969077246, 'criterion': 'mse', 'max_depth': 24, 'n_estimators': 301, 'max_features': 0.2558396564725872}. Best is trial 7 with value: 395.06265119225293.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:55:57,062]\u001b[0m Trial 12 finished with value: 377.34029713964713 and parameters: {'min_samples_split': 0.036182010962502203, 'criterion': 'mse', 'max_depth': 26, 'n_estimators': 277, 'max_features': 0.23192389958250165}. Best is trial 12 with value: 377.34029713964713.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:56:27,165]\u001b[0m Trial 13 finished with value: 374.1124826710575 and parameters: {'min_samples_split': 0.00936545747360128, 'criterion': 'mse', 'max_depth': 32, 'n_estimators': 275, 'max_features': 0.16460006062257704}. Best is trial 13 with value: 374.1124826710575.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:56:43,995]\u001b[0m Trial 14 finished with value: 388.93183779569426 and parameters: {'min_samples_split': 0.07871325591132088, 'criterion': 'mse', 'max_depth': 32, 'n_estimators': 290, 'max_features': 0.1357594037826277}. Best is trial 13 with value: 374.1124826710575.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:57:08,485]\u001b[0m Trial 15 finished with value: 375.8090368712763 and parameters: {'min_samples_split': 0.02684802538501857, 'criterion': 'mse', 'max_depth': 26, 'n_estimators': 355, 'max_features': 0.12316137641141889}. Best is trial 13 with value: 374.1124826710575.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:57:12,193]\u001b[0m Trial 16 finished with value: 378.3707357797126 and parameters: {'min_samples_split': 0.026674021788024083, 'criterion': 'mse', 'max_depth': 27, 'n_estimators': 395, 'max_features': 0.014777935012481286}. Best is trial 13 with value: 374.1124826710575.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:57:21,857]\u001b[0m Trial 17 finished with value: 397.5439061898215 and parameters: {'min_samples_split': 0.2960844940221135, 'criterion': 'mse', 'max_depth': 32, 'n_estimators': 347, 'max_features': 0.09395700144565454}. Best is trial 13 with value: 374.1124826710575.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:57:52,507]\u001b[0m Trial 18 finished with value: 373.9631822868633 and parameters: {'min_samples_split': 0.0013366940017597168, 'criterion': 'mse', 'max_depth': 22, 'n_estimators': 250, 'max_features': 0.16181402804341785}. Best is trial 18 with value: 373.9631822868633.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:57:52,785]\u001b[0m Trial 19 finished with value: 668.356145515394 and parameters: {'min_samples_split': 0.8449242810561082, 'criterion': 'mse', 'max_depth': 9, 'n_estimators': 245, 'max_features': 0.16554742746330942}. Best is trial 18 with value: 373.9631822868633.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random_state)\n",
    "def random_forest_objective(trial):\n",
    "    min_samples_split = trial.suggest_uniform('min_samples_split', 0+sys.float_info.min, 1)\n",
    "    criterion = trial.suggest_categorical('criterion', ['mse','mae'])\n",
    "    max_depth = trial.suggest_int('max_depth', min_max_depth, max_max_depth)\n",
    "    n_estimators = trial.suggest_int('n_estimators', min_n_estimators, max_n_estimators)\n",
    "    max_features = trial.suggest_uniform('max_features', 0+sys.float_info.min, 0.6)\n",
    "\n",
    "    clf = ensemble.RandomForestRegressor(\n",
    "        random_state=random_state,\n",
    "        min_samples_split=min_samples_split,\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        max_features=max_features\n",
    "        )\n",
    "    clf = clf.fit(x_train, y_train)\n",
    "    y_val_pred = clf.predict(x_val)\n",
    "    return math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "rf_optuna = optuna.create_study(direction='minimize')\n",
    "start_time = time.time()\n",
    "rf_optuna.optimize(random_forest_objective, n_trials=budget, n_jobs=n_jobs)\n",
    "end_time = time.time()\n",
    "\n",
    "summary['random_forest'] = summary['random_forest'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': rf_optuna.best_value,\n",
    "    'Min. samples split': rf_optuna.best_params['min_samples_split'], \n",
    "    'Criterion': rf_optuna.best_params['criterion'], \n",
    "    'Max. depth': rf_optuna.best_params['max_depth'],\n",
    "    'N. estimators': rf_optuna.best_params['n_estimators'],\n",
    "    'Max. features': rf_optuna.best_params['max_features']\n",
    "    },\n",
    "    name='optuna'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Gradient Boosting\n",
    "\n",
    "In this section we seek to apply Gradient Boosting. Boosting merely wants to boost weak models (as are our trees) through ensembles by sequentially adding a new model to that ensemble with the idea that every model added to the ensemble might do better than the last.\n",
    "\n",
    "Gradient Boosting takes this idea of Boosting and applies it through Gradient Descent. Basically, every added model will approximate the distance between the ensembles output at that iteration and the actual output we want to get. And by adding new models to our ensembles we expect that difference of outputs to decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Default hyper-parameters\n",
    "\n",
    "Here we first implement Gradient Boosting using the scikit-learn library with default hyper-parameters. \n",
    "\n",
    "Note how, since we are not only working with regression trees but gradient descent, apart from hyper-parameters for the trees we now find hyper-parameters like *learning rate* that are used to determine the steplegnth in the descent direction towards the optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation using sklearn\n",
    "np.random.seed(random_state)\n",
    "gb_sk_def = ensemble.GradientBoostingRegressor(random_state=random_state, verbose=verbose)\n",
    "\n",
    "start_time = time.time()\n",
    "gb_sk_def = gb_sk_def.fit(x_train, y_train)\n",
    "y_val_pred = gb_sk_def.predict(x_val)\n",
    "score =  math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "end_time = time.time()\n",
    "\n",
    "summary['gradient_boosting'] = summary['gradient_boosting'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': score,\n",
    "    'Learning rate': 0.1,\n",
    "    'N. estimators': 100,\n",
    "    'Criterion': 'friedman_mse', \n",
    "    'Min. samples split': 2, \n",
    "    'Min. samples leaf': 1,\n",
    "    'Max. depth': 3,\n",
    "    'Max. leaf nodes': 'None'\n",
    "    },\n",
    "    name='default'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also implemented Gradient Boosting using the XGBoost library.\n",
    " \n",
    "The implementation is pratically the same, only having to transform or cast the input training and test matrices to the libraries matrices.\n",
    "Regarding the hyper-parameters, some that might be present in scikit are not in xgb or vice-versa but let us implement Gradient Descent this way and later on evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation using xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "gb_xgb_def = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "start_time = time.time()\n",
    "gb_xgb_def = gb_xgb_def.fit(x_train, y_train)\n",
    "y_val_pred = gb_xgb_def.predict(x_val)\n",
    "score = math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "end_time = time.time()\n",
    "\n",
    "summary['gradient_boosting'] = summary['gradient_boosting'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': score,\n",
    "    'Learning rate': 0.3,\n",
    "    'Max. depth': 6,\n",
    "    'Max. leaf nodes': 0,\n",
    "    'Gamma (min_split_loss)': 0,\n",
    "    'Lambda': 1,\n",
    "    'Alpha': 0,\n",
    "    'N. estimators': gb_xgb_def.get_params()['n_estimators']\n",
    "    },\n",
    "    name='default_xgboost'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Hyper-parameter tunning\n",
    "\n",
    "As we did in the last subsection we will now implement Gradient Boosting for both scikit-learn and XGBoost tuning their hyper-parameters through Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_leaf_nodes = 2\n",
    "max_max_leaf_nodes = 20\n",
    "min_min_samples_leaf = 1\n",
    "max_min_samples_leaf = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn the execution took quite a while which is why we created a variable *short* to control how many hyper-parameters to train. However, the outputs execution you see below corresponds to training all hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-01-14 18:11:26,619]\u001b[0m A new study created in memory with name: no-name-038d6898-2561-4001-92f3-e55ab327c2a7\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:13:23,055]\u001b[0m Trial 0 finished with value: 426.64002717133434 and parameters: {'learning_rate': 0.5065337886424471, 'n_estimators': 150, 'min_samples_split': 0.28252873137478773, 'max_depth': 27, 'criterion': 'friedman_mse', 'min_samples_leaf': 4, 'max_leaf_nodes': 11}. Best is trial 0 with value: 426.64002717133434.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:15:59,022]\u001b[0m Trial 1 finished with value: 498.9519962473219 and parameters: {'learning_rate': 0.9314245286134138, 'n_estimators': 212, 'min_samples_split': 0.30320790912098206, 'max_depth': 27, 'criterion': 'friedman_mse', 'min_samples_leaf': 4, 'max_leaf_nodes': 11}. Best is trial 0 with value: 426.64002717133434.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:17:31,351]\u001b[0m Trial 2 finished with value: 379.6951509882391 and parameters: {'learning_rate': 0.04766293389176224, 'n_estimators': 219, 'min_samples_split': 0.6676483839276645, 'max_depth': 26, 'criterion': 'mse', 'min_samples_leaf': 3, 'max_leaf_nodes': 12}. Best is trial 2 with value: 379.6951509882391.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:19:51,119]\u001b[0m Trial 3 finished with value: 406.1053610905662 and parameters: {'learning_rate': 0.299410392579339, 'n_estimators': 257, 'min_samples_split': 0.3137779302022752, 'max_depth': 19, 'criterion': 'mse', 'min_samples_leaf': 1, 'max_leaf_nodes': 7}. Best is trial 2 with value: 379.6951509882391.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:20:34,831]\u001b[0m Trial 4 finished with value: 399.3872407603477 and parameters: {'learning_rate': 0.27884504117709263, 'n_estimators': 224, 'min_samples_split': 0.018318927209558744, 'max_depth': 30, 'criterion': 'mse', 'min_samples_leaf': 1, 'max_leaf_nodes': 2}. Best is trial 2 with value: 379.6951509882391.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:21:05,563]\u001b[0m Trial 5 finished with value: 379.70087664350984 and parameters: {'learning_rate': 0.05160148340684523, 'n_estimators': 64, 'min_samples_split': 0.4771808656230009, 'max_depth': 11, 'criterion': 'mse', 'min_samples_leaf': 7, 'max_leaf_nodes': 16}. Best is trial 2 with value: 379.6951509882391.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:23:15,538]\u001b[0m Trial 6 finished with value: 483.6270071359956 and parameters: {'learning_rate': 0.896271169988134, 'n_estimators': 209, 'min_samples_split': 0.3895318348476231, 'max_depth': 15, 'criterion': 'mse', 'min_samples_leaf': 4, 'max_leaf_nodes': 9}. Best is trial 2 with value: 379.6951509882391.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:24:17,124]\u001b[0m Trial 7 finished with value: 474.93462549236864 and parameters: {'learning_rate': 0.823530522858172, 'n_estimators': 130, 'min_samples_split': 0.47797280614623383, 'max_depth': 6, 'criterion': 'friedman_mse', 'min_samples_leaf': 2, 'max_leaf_nodes': 17}. Best is trial 2 with value: 379.6951509882391.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:26:19,355]\u001b[0m Trial 8 finished with value: 378.2144631304824 and parameters: {'learning_rate': 0.04746706686824942, 'n_estimators': 329, 'min_samples_split': 0.054990279308563705, 'max_depth': 4, 'criterion': 'mse', 'min_samples_leaf': 9, 'max_leaf_nodes': 16}. Best is trial 8 with value: 378.2144631304824.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:27:24,563]\u001b[0m Trial 9 finished with value: 394.58779986971905 and parameters: {'learning_rate': 0.16203767546861447, 'n_estimators': 349, 'min_samples_split': 0.42449476452416246, 'max_depth': 27, 'criterion': 'friedman_mse', 'min_samples_leaf': 3, 'max_leaf_nodes': 2}. Best is trial 8 with value: 378.2144631304824.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:28:08,392]\u001b[0m Trial 10 finished with value: 407.75755975473567 and parameters: {'learning_rate': 0.5807903793078439, 'n_estimators': 399, 'min_samples_split': 0.986920039689828, 'max_depth': 2, 'criterion': 'mse', 'min_samples_leaf': 10, 'max_leaf_nodes': 20}. Best is trial 8 with value: 378.2144631304824.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:29:44,837]\u001b[0m Trial 11 finished with value: 378.7885549788847 and parameters: {'learning_rate': 0.04688778597925469, 'n_estimators': 307, 'min_samples_split': 0.7553222593465958, 'max_depth': 21, 'criterion': 'mse', 'min_samples_leaf': 8, 'max_leaf_nodes': 15}. Best is trial 8 with value: 378.2144631304824.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:31:42,075]\u001b[0m Trial 12 finished with value: 396.8105245098501 and parameters: {'learning_rate': 0.25707173589941934, 'n_estimators': 318, 'min_samples_split': 0.7604436658210907, 'max_depth': 20, 'criterion': 'mse', 'min_samples_leaf': 9, 'max_leaf_nodes': 15}. Best is trial 8 with value: 378.2144631304824.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:35:12,482]\u001b[0m Trial 13 finished with value: 664.1075424765118 and parameters: {'learning_rate': 3.1955347117695876e-05, 'n_estimators': 305, 'min_samples_split': 0.01996691074966958, 'max_depth': 11, 'criterion': 'mse', 'min_samples_leaf': 8, 'max_leaf_nodes': 20}. Best is trial 8 with value: 378.2144631304824.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:36:23,053]\u001b[0m Trial 14 finished with value: 428.0518105429215 and parameters: {'learning_rate': 0.6877994563398588, 'n_estimators': 396, 'min_samples_split': 0.9298721767726295, 'max_depth': 22, 'criterion': 'mse', 'min_samples_leaf': 7, 'max_leaf_nodes': 14}. Best is trial 8 with value: 378.2144631304824.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:38:48,758]\u001b[0m Trial 15 finished with value: 387.04860479667695 and parameters: {'learning_rate': 0.1689695927705262, 'n_estimators': 344, 'min_samples_split': 0.70425950279546, 'max_depth': 14, 'criterion': 'mse', 'min_samples_leaf': 10, 'max_leaf_nodes': 19}. Best is trial 8 with value: 378.2144631304824.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:39:34,405]\u001b[0m Trial 16 finished with value: 404.79998506944287 and parameters: {'learning_rate': 0.3948517612664872, 'n_estimators': 272, 'min_samples_split': 0.8394776506391941, 'max_depth': 2, 'criterion': 'mse', 'min_samples_leaf': 6, 'max_leaf_nodes': 18}. Best is trial 8 with value: 378.2144631304824.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:42:46,050]\u001b[0m Trial 17 finished with value: 379.77485371140745 and parameters: {'learning_rate': 0.113706437257947, 'n_estimators': 376, 'min_samples_split': 0.5760999007273127, 'max_depth': 7, 'criterion': 'mse', 'min_samples_leaf': 9, 'max_leaf_nodes': 14}. Best is trial 8 with value: 378.2144631304824.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:45:39,736]\u001b[0m Trial 18 finished with value: 412.0758088713627 and parameters: {'learning_rate': 0.4072116620641126, 'n_estimators': 288, 'min_samples_split': 0.11221882716848963, 'max_depth': 22, 'criterion': 'mse', 'min_samples_leaf': 8, 'max_leaf_nodes': 8}. Best is trial 8 with value: 378.2144631304824.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:46:44,456]\u001b[0m Trial 19 finished with value: 386.74846751087114 and parameters: {'learning_rate': 0.028938625916631378, 'n_estimators': 335, 'min_samples_split': 0.8546933112189022, 'max_depth': 16, 'criterion': 'mse', 'min_samples_leaf': 6, 'max_leaf_nodes': 13}. Best is trial 8 with value: 378.2144631304824.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# hyperparam tuning for sklearn ensemble.GradientBoostingRegressor\n",
    "np.random.seed(random_state)\n",
    "\n",
    "def gradboosting_objective(trial):  \n",
    "    gb_sk_opt = None\n",
    "    short = False\n",
    "    \n",
    "    learning_rate = trial.suggest_uniform('learning_rate', 0+sys.float_info.min, 1)\n",
    "    n_estimators = trial.suggest_int('n_estimators', min_n_estimators, max_n_estimators)\n",
    "    min_samples_split = trial.suggest_uniform('min_samples_split', 0+sys.float_info.min, 1)\n",
    "    max_depth = trial.suggest_int('max_depth', min_max_depth, max_max_depth)\n",
    "        \n",
    "    if short == False: # it will take a long time to run \n",
    "        criterion = trial.suggest_categorical('criterion', ['mse','friedman_mse'])\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf',min_min_samples_leaf, max_min_samples_leaf)\n",
    "        max_leaf_nodes = trial.suggest_int('max_leaf_nodes', min_max_leaf_nodes, max_max_leaf_nodes)\n",
    "        \n",
    "        gb_sk_opt = ensemble.GradientBoostingRegressor(learning_rate=learning_rate, \n",
    "                                                   n_estimators=n_estimators,\n",
    "                                                   criterion=criterion,\n",
    "                                                   min_samples_split=min_samples_split,\n",
    "                                                   min_samples_leaf=min_samples_leaf,\n",
    "                                                   max_depth=max_depth,\n",
    "                                                   max_leaf_nodes=max_leaf_nodes,\n",
    "                                                   random_state=random_state,\n",
    "                                                   verbose=verbose)\n",
    "    else:  # will take less time        \n",
    "        gb_sk_opt = ensemble.GradientBoostingRegressor(learning_rate=learning_rate, \n",
    "                                                   n_estimators=n_estimators,\n",
    "                                                   min_samples_split=min_samples_split,\n",
    "                                                   max_depth=max_depth,\n",
    "                                                   random_state=random_state,\n",
    "                                                   verbose=verbose)\n",
    "        \n",
    "    gb_sk_opt = gb_sk_opt.fit(x_train, y_train)\n",
    "    y_val_pred = gb_sk_opt.predict(x_val)\n",
    "    \n",
    "    return math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "gb_optuna = optuna.create_study(direction='minimize')\n",
    "start_time = time.time()\n",
    "gb_optuna.optimize(gradboosting_objective, n_trials=budget)\n",
    "end_time = time.time()\n",
    "\n",
    "summary['gradient_boosting'] = summary['gradient_boosting'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': gb_optuna.best_value,\n",
    "    'Learning rate': gb_optuna.best_params['learning_rate'],\n",
    "    'N. estimators': gb_optuna.best_params['n_estimators'],\n",
    "    'Criterion': 'friedman_mse', \n",
    "    'Min. samples leaf': gb_optuna.best_params['min_samples_leaf'],\n",
    "    'Min. samples leaf': 1,\n",
    "    'Max. depth': gb_optuna.best_params['max_depth'],\n",
    "    'Max. leaf nodes': gb_optuna.best_params['max_leaf_nodes']\n",
    "    },\n",
    "    name='optuna_sklearn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning XGBRegressor with Optuna the execution takes less time so there was no need to take a similar approach to what we did above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-01-14 18:53:05,160]\u001b[0m A new study created in memory with name: no-name-12fcd1a4-66f2-4b4b-8f6b-144c43a05c92\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:54:08,872]\u001b[0m Trial 0 finished with value: 445.11596138786507 and parameters: {'eta': 0.5475110908996418, 'max_depth': 12, 'n_estimators': 399, 'gamma': 0.9945206996717629, 'lambda': 0.16947785185193312, 'alpha': 0.17868598835093621}. Best is trial 0 with value: 445.11596138786507.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:55:24,697]\u001b[0m Trial 1 finished with value: 431.4641278948212 and parameters: {'eta': 0.4029138842455492, 'max_depth': 25, 'n_estimators': 235, 'gamma': 0.9927485201217258, 'lambda': 0.2773160972893963, 'alpha': 0.3910011528521415}. Best is trial 1 with value: 431.4641278948212.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:56:12,182]\u001b[0m Trial 2 finished with value: 489.20321514411506 and parameters: {'eta': 0.7623333814899286, 'max_depth': 30, 'n_estimators': 150, 'gamma': 0.7825333606141713, 'lambda': 0.4279957296676617, 'alpha': 0.0620228490000086}. Best is trial 1 with value: 431.4641278948212.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:57:45,231]\u001b[0m Trial 3 finished with value: 455.7544344692287 and parameters: {'eta': 0.550723372477111, 'max_depth': 31, 'n_estimators': 298, 'gamma': 0.08229083750511458, 'lambda': 0.48769546593359003, 'alpha': 0.4889142580741816}. Best is trial 1 with value: 431.4641278948212.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:58:27,083]\u001b[0m Trial 4 finished with value: 492.41203785303384 and parameters: {'eta': 0.7951927720818436, 'max_depth': 13, 'n_estimators': 183, 'gamma': 0.19144717198107555, 'lambda': 0.09629643720843441, 'alpha': 0.38276233155492123}. Best is trial 1 with value: 431.4641278948212.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 18:59:28,184]\u001b[0m Trial 5 finished with value: 445.3363741201978 and parameters: {'eta': 0.5779104207855955, 'max_depth': 12, 'n_estimators': 343, 'gamma': 0.06338097147049937, 'lambda': 0.271614585870421, 'alpha': 0.09310453144424341}. Best is trial 1 with value: 431.4641278948212.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 19:00:12,004]\u001b[0m Trial 6 finished with value: 425.9101596953247 and parameters: {'eta': 0.35852407239354644, 'max_depth': 26, 'n_estimators': 139, 'gamma': 0.3796062041286333, 'lambda': 0.2551020171248045, 'alpha': 0.1491983271408614}. Best is trial 6 with value: 425.9101596953247.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 19:00:15,312]\u001b[0m Trial 7 finished with value: 471.8172025267326 and parameters: {'eta': 0.7227641344952237, 'max_depth': 4, 'n_estimators': 61, 'gamma': 0.8791774495169711, 'lambda': 0.37065631725687326, 'alpha': 0.3415545667149384}. Best is trial 6 with value: 425.9101596953247.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 19:01:11,853]\u001b[0m Trial 8 finished with value: 456.1802642273191 and parameters: {'eta': 0.010571888934746587, 'max_depth': 31, 'n_estimators': 173, 'gamma': 0.2770786342417741, 'lambda': 0.050641044065492406, 'alpha': 0.17126344276313593}. Best is trial 6 with value: 425.9101596953247.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 19:02:04,241]\u001b[0m Trial 9 finished with value: 439.7067259587051 and parameters: {'eta': 0.15205373215211326, 'max_depth': 17, 'n_estimators': 230, 'gamma': 0.9881041264834598, 'lambda': 0.04529050479684297, 'alpha': 0.028214444757918816}. Best is trial 6 with value: 425.9101596953247.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 19:02:19,014]\u001b[0m Trial 10 finished with value: 425.47901687461274 and parameters: {'eta': 0.2747781456793045, 'max_depth': 22, 'n_estimators': 55, 'gamma': 0.5171753419477968, 'lambda': 0.1825598774564554, 'alpha': 0.2533829752391293}. Best is trial 10 with value: 425.47901687461274.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 19:02:33,213]\u001b[0m Trial 11 finished with value: 420.67147511206514 and parameters: {'eta': 0.27943350051242405, 'max_depth': 22, 'n_estimators': 53, 'gamma': 0.5141744893299047, 'lambda': 0.18748587463202532, 'alpha': 0.24734225027585072}. Best is trial 11 with value: 420.67147511206514.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 19:02:46,964]\u001b[0m Trial 12 finished with value: 428.0115540194387 and parameters: {'eta': 0.2557267993216589, 'max_depth': 22, 'n_estimators': 52, 'gamma': 0.5826727999446332, 'lambda': 0.1813692042693552, 'alpha': 0.25688745386100287}. Best is trial 11 with value: 420.67147511206514.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 19:03:13,000]\u001b[0m Trial 13 finished with value: 769.3152727282904 and parameters: {'eta': 0.0031808527660032726, 'max_depth': 20, 'n_estimators': 101, 'gamma': 0.5749435865077861, 'lambda': 0.16697957262375943, 'alpha': 0.27235328168552214}. Best is trial 11 with value: 420.67147511206514.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 19:03:29,225]\u001b[0m Trial 14 finished with value: 413.10499734186953 and parameters: {'eta': 0.1733189284595858, 'max_depth': 25, 'n_estimators': 53, 'gamma': 0.45886353661605034, 'lambda': 0.3302383063496657, 'alpha': 0.2910324528211042}. Best is trial 14 with value: 413.10499734186953.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 19:03:58,321]\u001b[0m Trial 15 finished with value: 409.372505076543 and parameters: {'eta': 0.10346547609955672, 'max_depth': 26, 'n_estimators': 97, 'gamma': 0.7029023240167427, 'lambda': 0.34752099909874873, 'alpha': 0.31603042548751453}. Best is trial 15 with value: 409.372505076543.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 19:04:31,315]\u001b[0m Trial 16 finished with value: 404.96192140417884 and parameters: {'eta': 0.10145263562603968, 'max_depth': 27, 'n_estimators': 105, 'gamma': 0.6985383792491521, 'lambda': 0.3491100563545699, 'alpha': 0.49545111922059837}. Best is trial 16 with value: 404.96192140417884.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 19:05:02,845]\u001b[0m Trial 17 finished with value: 404.73402303213544 and parameters: {'eta': 0.07598342413008832, 'max_depth': 28, 'n_estimators': 105, 'gamma': 0.7347167779663257, 'lambda': 0.41829679645872875, 'alpha': 0.4939364209822521}. Best is trial 17 with value: 404.73402303213544.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 19:05:31,020]\u001b[0m Trial 18 finished with value: 692.9324791899229 and parameters: {'eta': 0.004825725739980288, 'max_depth': 29, 'n_estimators': 100, 'gamma': 0.6824249773708858, 'lambda': 0.4916072595434766, 'alpha': 0.4957935146134179}. Best is trial 17 with value: 404.73402303213544.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 19:05:36,216]\u001b[0m Trial 19 finished with value: 477.9842506049847 and parameters: {'eta': 0.8980895483444205, 'max_depth': 2, 'n_estimators': 196, 'gamma': 0.8292131563051404, 'lambda': 0.4229923261598012, 'alpha': 0.4484804942931187}. Best is trial 17 with value: 404.73402303213544.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# hyperparam tuning for XGBoost Regressor\n",
    "def xgradboosting_objective(trial):\n",
    "    \n",
    "    eta = trial.suggest_uniform('eta', 0+sys.float_info.min, 1.0)\n",
    "    max_depth = trial.suggest_int('max_depth', min_max_depth, max_max_depth)\n",
    "    n_estimators = trial.suggest_int('n_estimators', min_n_estimators, max_n_estimators)\n",
    "    \n",
    "    gamma = trial.suggest_float('gamma', 0.01, 1.0)\n",
    "    reg_lambda = trial.suggest_uniform('lambda', 0.01, 0.5)\n",
    "    reg_alpha = trial.suggest_uniform('alpha', 0.01, 0.5)\n",
    "\n",
    "    gb_xgb_opt = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                                  booster='gbtree',\n",
    "                                  learning_rate=eta,\n",
    "                                  gamma=gamma,\n",
    "                                  reg_alpha=reg_alpha,\n",
    "                                  reg_lambda=reg_lambda,\n",
    "                                  max_depth=max_depth,\n",
    "                                  n_estimators=n_estimators,\n",
    "                                  random_state=random_state,\n",
    "                                  verbosity=verbose\n",
    "                                 )\n",
    "\n",
    "    gb_xgb_opt = gb_xgb_opt.fit(x_train, y_train)\n",
    "    y_val_pred = gb_xgb_opt.predict(x_val)\n",
    "    \n",
    "    return math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "\n",
    "gb_optuna = optuna.create_study(direction='minimize')\n",
    "start_time = time.time()\n",
    "gb_optuna.optimize(xgradboosting_objective, n_trials=budget)\n",
    "end_time = time.time()\n",
    "\n",
    "summary['gradient_boosting'] = summary['gradient_boosting'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': gb_optuna.best_value,\n",
    "    'Learning rate': gb_optuna.best_params['eta'],\n",
    "    'Max. depth': gb_optuna.best_params['max_depth'],\n",
    "    'Gamma (min_split_loss)': gb_optuna.best_params['gamma'],\n",
    "    'Lambda': gb_optuna.best_params['lambda'],\n",
    "    'Alpha': gb_optuna.best_params['alpha'],\n",
    "    'N. estimators': gb_optuna.best_params['n_estimators']  \n",
    "    },\n",
    "    name='optuna_xgboost'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Let us take a look at the results. Generally speaking Optuna takes a longer time to execute but in the upside, also obtains better scores.\n",
    "\n",
    "Comparing between the different methods, clearly knn is the worst in terms of scores but time wise, the default is a lot faster. \n",
    "Random Forest and Optuna tuned Scikit Gradient Boosting seem to get similar scores, however, that specific Gradient Boosting implementation takes a very long time to run whereas Random Forest gets one of the best scores in less time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time (sec)</th>\n",
       "      <th>Score (RMSE)</th>\n",
       "      <th>N. neighbors</th>\n",
       "      <th>Weights</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>default</th>\n",
       "      <td>0.1151</td>\n",
       "      <td>455.123868</td>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optuna</th>\n",
       "      <td>31.7793</td>\n",
       "      <td>425.084222</td>\n",
       "      <td>12</td>\n",
       "      <td>distance</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time (sec)  Score (RMSE) N. neighbors   Weights  P\n",
       "default     0.1151    455.123868            5   uniform  2\n",
       "optuna     31.7793    425.084222           12  distance  1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary['knn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time (sec)</th>\n",
       "      <th>Score (RMSE)</th>\n",
       "      <th>Min. samples split</th>\n",
       "      <th>Criterion</th>\n",
       "      <th>Max. depth</th>\n",
       "      <th>N. estimators</th>\n",
       "      <th>Max. features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>default</th>\n",
       "      <td>78.0287</td>\n",
       "      <td>376.878001</td>\n",
       "      <td>2</td>\n",
       "      <td>mse</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optuna</th>\n",
       "      <td>1385.7564</td>\n",
       "      <td>373.963182</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>mse</td>\n",
       "      <td>22</td>\n",
       "      <td>250</td>\n",
       "      <td>0.161814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time (sec)  Score (RMSE) Min. samples split Criterion Max. depth  \\\n",
       "default    78.0287    376.878001                  2       mse       None   \n",
       "optuna   1385.7564    373.963182           0.001337       mse         22   \n",
       "\n",
       "        N. estimators Max. features  \n",
       "default           100             1  \n",
       "optuna            250      0.161814  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary['random_forest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time (sec)</th>\n",
       "      <th>Score (RMSE)</th>\n",
       "      <th>Criterion</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Max. depth</th>\n",
       "      <th>Max. leaf nodes</th>\n",
       "      <th>Min. samples leaf</th>\n",
       "      <th>Min. samples split</th>\n",
       "      <th>N. estimators</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Gamma (min_split_loss)</th>\n",
       "      <th>Lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>default</th>\n",
       "      <td>29.0407</td>\n",
       "      <td>389.223359</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optuna_sklearn</th>\n",
       "      <td>2117.8360</td>\n",
       "      <td>378.214463</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.047467</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>329.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default_xgboost</th>\n",
       "      <td>7.9902</td>\n",
       "      <td>409.802870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optuna_xgboost</th>\n",
       "      <td>751.0554</td>\n",
       "      <td>404.734023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075983</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.493936</td>\n",
       "      <td>0.734717</td>\n",
       "      <td>0.418297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time (sec)  Score (RMSE)     Criterion  Learning rate  \\\n",
       "default            29.0407    389.223359  friedman_mse       0.100000   \n",
       "optuna_sklearn   2117.8360    378.214463  friedman_mse       0.047467   \n",
       "default_xgboost     7.9902    409.802870           NaN       0.300000   \n",
       "optuna_xgboost    751.0554    404.734023           NaN       0.075983   \n",
       "\n",
       "                 Max. depth Max. leaf nodes  Min. samples leaf  \\\n",
       "default                 3.0            None                1.0   \n",
       "optuna_sklearn          4.0              16                1.0   \n",
       "default_xgboost         6.0               0                NaN   \n",
       "optuna_xgboost         28.0             NaN                NaN   \n",
       "\n",
       "                 Min. samples split  N. estimators     Alpha  \\\n",
       "default                         2.0          100.0       NaN   \n",
       "optuna_sklearn                  NaN          329.0       NaN   \n",
       "default_xgboost                 NaN          100.0  0.000000   \n",
       "optuna_xgboost                  NaN          105.0  0.493936   \n",
       "\n",
       "                 Gamma (min_split_loss)    Lambda  \n",
       "default                             NaN       NaN  \n",
       "optuna_sklearn                      NaN       NaN  \n",
       "default_xgboost                0.000000  1.000000  \n",
       "optuna_xgboost                 0.734717  0.418297  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary['gradient_boosting']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will estimate the performance of our best model, in this case, random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_optuna = ensemble.RandomForestRegressor(\n",
    "          random_state=random_state,\n",
    "          min_samples_split=rf_optuna.best_params['min_samples_split'],\n",
    "          criterion=rf_optuna.best_params['criterion'],\n",
    "          max_depth=rf_optuna.best_params['max_depth'],\n",
    "          n_estimators=rf_optuna.best_params['n_estimators'],\n",
    "          max_features=rf_optuna.best_params['max_features']\n",
    "      )\n",
    "rf_best_optuna = rf_best_optuna.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attribute selection model performance:\n",
      "RMSE: 383.0463 (should be lower than the trivial predictor using the mean MSE: 689.8476)\n",
      "R square: 0.6917 (should be higher than the trivial predictor using the mean: R square 0.0000)\n"
     ]
    }
   ],
   "source": [
    "rf_opt_predict = rf_best_optuna.predict(x_test)\n",
    "print('\\nAttribute selection model performance:')\n",
    "print('RMSE: {:.4f} (should be lower than the trivial predictor using the mean MSE: {:.4f})'.format(\n",
    "    math.sqrt(metrics.mean_squared_error(y_test, rf_opt_predict)),\n",
    "    math.sqrt(metrics.mean_squared_error(y_test, [y_test.mean() for i in range(len(y_test))]))))\n",
    "print('R square: {:.4f} (should be higher than the trivial predictor using the mean: R square {:.4f})'.format(\n",
    "    metrics.r2_score(y_test, rf_opt_predict),\n",
    "    metrics.r2_score(y_test, [y_test.mean() for i in range(len(y_test))])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ATTRIBUTE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Select from all attributes\n",
    "\n",
    "**Are all 550 input attributes actually necessary in order to get a good model? Is it possible to have an accurate model that uses fewer than 550 variables? How many?**\n",
    "\n",
    "For this question we will be using the random forest as in previous sections, but now we will include the parameter for select only certain attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_depth = 2\n",
    "max_max_depth = 25\n",
    "min_n_estimators = 50\n",
    "max_n_estimators = 300\n",
    "min_n_k = 10\n",
    "max_n_k = 550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate whether all 550 attributes are necessary to obtain a good model, we add a new hyper-parameter to the optuna tuning objective function *k*. This hyper-parameter represents the number of attributes needed to obtain a good model. As we want to achieve at least some dimension reduction, we will set the maximum number of attributes to use at 350 of the 550 we had available. (33% reduction)\n",
    "\n",
    "In order to do both feature selection and regression, we create a pipeline that allows us to first select the best k features and once the attributes have been decided upon, apply regression just as we did in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-01-14 17:00:12,137]\u001b[0m A new study created in memory with name: no-name-265ab6f7-48ef-4f90-8293-9335db6c1b5f\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:02:31,846]\u001b[0m Trial 0 finished with value: 551.4198926282447 and parameters: {'k': 324, 'min_samples_split': 0.5865215732384921, 'criterion': 'mae', 'max_depth': 12, 'n_estimators': 179, 'max_features': 0.49862808139915354}. Best is trial 0 with value: 551.4198926282447.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:02:32,571]\u001b[0m Trial 1 finished with value: 493.95580466376583 and parameters: {'k': 86, 'min_samples_split': 0.4608837235993515, 'criterion': 'mse', 'max_depth': 2, 'n_estimators': 93, 'max_features': 0.37201623130564365}. Best is trial 1 with value: 493.95580466376583.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:02:33,559]\u001b[0m Trial 2 finished with value: 495.557605619882 and parameters: {'k': 116, 'min_samples_split': 0.38793609024141273, 'criterion': 'mse', 'max_depth': 2, 'n_estimators': 96, 'max_features': 0.3738917205994864}. Best is trial 1 with value: 493.95580466376583.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:02:33,837]\u001b[0m Trial 3 finished with value: 668.3672229162105 and parameters: {'k': 375, 'min_samples_split': 0.9405700373745043, 'criterion': 'mse', 'max_depth': 19, 'n_estimators': 237, 'max_features': 0.5767668031998159}. Best is trial 1 with value: 493.95580466376583.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:06:04,763]\u001b[0m Trial 4 finished with value: 550.126274563272 and parameters: {'k': 456, 'min_samples_split': 0.5821868279200875, 'criterion': 'mae', 'max_depth': 25, 'n_estimators': 202, 'max_features': 0.4614846286300733}. Best is trial 1 with value: 493.95580466376583.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:06:11,707]\u001b[0m Trial 5 finished with value: 450.14583089778324 and parameters: {'k': 184, 'min_samples_split': 0.25176008213787904, 'criterion': 'mse', 'max_depth': 4, 'n_estimators': 272, 'max_features': 0.3662181929670834}. Best is trial 5 with value: 450.14583089778324.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:06:11,796]\u001b[0m Trial 6 finished with value: 668.4876515201565 and parameters: {'k': 203, 'min_samples_split': 0.7833009880073841, 'criterion': 'mse', 'max_depth': 3, 'n_estimators': 56, 'max_features': 0.4321521113315954}. Best is trial 5 with value: 450.14583089778324.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:10:49,698]\u001b[0m Trial 7 finished with value: 473.4295041642436 and parameters: {'k': 485, 'min_samples_split': 0.2868174708423191, 'criterion': 'mae', 'max_depth': 2, 'n_estimators': 139, 'max_features': 0.4629324380771608}. Best is trial 5 with value: 450.14583089778324.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:12:02,118]\u001b[0m Trial 8 finished with value: 454.17005348993416 and parameters: {'k': 308, 'min_samples_split': 0.4794282020023456, 'criterion': 'mae', 'max_depth': 3, 'n_estimators': 158, 'max_features': 0.15488805460260435}. Best is trial 5 with value: 450.14583089778324.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:12:02,394]\u001b[0m Trial 9 finished with value: 719.5841140488673 and parameters: {'k': 546, 'min_samples_split': 0.7324528465734474, 'criterion': 'mae', 'max_depth': 4, 'n_estimators': 73, 'max_features': 0.17613631764522608}. Best is trial 5 with value: 450.14583089778324.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:12:05,950]\u001b[0m Trial 10 finished with value: 409.0822933727605 and parameters: {'k': 180, 'min_samples_split': 0.017379526704949244, 'criterion': 'mse', 'max_depth': 8, 'n_estimators': 299, 'max_features': 0.07963781447950782}. Best is trial 10 with value: 409.0822933727605.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:12:06,809]\u001b[0m Trial 11 finished with value: 431.901223453622 and parameters: {'k': 181, 'min_samples_split': 0.012557612159452548, 'criterion': 'mse', 'max_depth': 8, 'n_estimators': 298, 'max_features': 0.014726215142943075}. Best is trial 10 with value: 409.0822933727605.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:12:07,463]\u001b[0m Trial 12 finished with value: 557.2466453478054 and parameters: {'k': 31, 'min_samples_split': 0.007368754918709586, 'criterion': 'mse', 'max_depth': 9, 'n_estimators': 300, 'max_features': 0.0020487774898828998}. Best is trial 10 with value: 409.0822933727605.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:12:08,109]\u001b[0m Trial 13 finished with value: 447.23416423552567 and parameters: {'k': 217, 'min_samples_split': 0.014032407005452479, 'criterion': 'mse', 'max_depth': 9, 'n_estimators': 299, 'max_features': 0.0018177448432917936}. Best is trial 10 with value: 409.0822933727605.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:12:10,304]\u001b[0m Trial 14 finished with value: 433.61349014884337 and parameters: {'k': 142, 'min_samples_split': 0.11513599816487027, 'criterion': 'mse', 'max_depth': 7, 'n_estimators': 256, 'max_features': 0.10386729281781089}. Best is trial 10 with value: 409.0822933727605.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:12:12,735]\u001b[0m Trial 15 finished with value: 409.56011233442166 and parameters: {'k': 240, 'min_samples_split': 0.12141672899946791, 'criterion': 'mse', 'max_depth': 17, 'n_estimators': 220, 'max_features': 0.06176604534458821}. Best is trial 10 with value: 409.0822933727605.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:12:21,296]\u001b[0m Trial 16 finished with value: 397.7033856348979 and parameters: {'k': 263, 'min_samples_split': 0.16767418716856797, 'criterion': 'mse', 'max_depth': 16, 'n_estimators': 223, 'max_features': 0.2218126021308216}. Best is trial 16 with value: 397.7033856348979.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:12:21,886]\u001b[0m Trial 17 finished with value: 608.8725339720969 and parameters: {'k': 14, 'min_samples_split': 0.17736484995067595, 'criterion': 'mse', 'max_depth': 13, 'n_estimators': 252, 'max_features': 0.2640381643484097}. Best is trial 16 with value: 397.7033856348979.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:12:29,259]\u001b[0m Trial 18 finished with value: 399.6462942410826 and parameters: {'k': 380, 'min_samples_split': 0.27885462086121154, 'criterion': 'mse', 'max_depth': 5, 'n_estimators': 197, 'max_features': 0.22735659670194813}. Best is trial 16 with value: 397.7033856348979.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:12:37,093]\u001b[0m Trial 19 finished with value: 399.7761114079588 and parameters: {'k': 383, 'min_samples_split': 0.32065367944766265, 'criterion': 'mse', 'max_depth': 5, 'n_estimators': 191, 'max_features': 0.243609944910457}. Best is trial 16 with value: 397.7033856348979.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744.9541549682617\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random_state)\n",
    "def random_forest_objective_attr(trial):\n",
    "    k = trial.suggest_int('k', min_n_k, max_n_k)\n",
    "    min_samples_split = trial.suggest_uniform('min_samples_split', 0+sys.float_info.min, 1)\n",
    "    criterion = trial.suggest_categorical('criterion', ['mse','mae'])\n",
    "    max_depth = trial.suggest_int('max_depth', min_max_depth, max_max_depth, log=True)\n",
    "    n_estimators = trial.suggest_int('n_estimators', min_n_estimators, max_n_estimators)\n",
    "    max_features = trial.suggest_uniform('max_features', 0+sys.float_info.min, 0.6)\n",
    "\n",
    "    clf = Pipeline([\n",
    "      ('feature_selection', feature_selection.SelectKBest(feature_selection.f_regression, k=k)),\n",
    "      ('regression', ensemble.RandomForestRegressor(\n",
    "          random_state=random_state,\n",
    "          min_samples_split=min_samples_split,\n",
    "          criterion=criterion,\n",
    "          max_depth=max_depth,\n",
    "          n_estimators=n_estimators,\n",
    "          max_features=max_features\n",
    "      ))\n",
    "    ])\n",
    "\n",
    "    clf = clf.fit(x_train, y_train)\n",
    "    y_val_pred = clf.predict(x_val)\n",
    "    return math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "rf_attr_optuna = optuna.create_study(direction='minimize')\n",
    "start_time = time.time()\n",
    "rf_attr_optuna.optimize(random_forest_objective_attr, n_trials=budget, n_jobs=n_jobs)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 263, 'min_samples_split': 0.16767418716856797, 'criterion': 'mse', 'max_depth': 16, 'n_estimators': 223, 'max_features': 0.2218126021308216} 397.7033856348979\n"
     ]
    }
   ],
   "source": [
    "print(rf_attr_optuna.best_params, rf_attr_optuna.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the results in terms of the RMSE are similar to the ones we get in the first section with random forest and gradient boosting, but using less amount of attributes. The RMSE is higher, but not a lot to the previous achieved. This means that we were using redundant information to train our models and we can use simpler models with the improvement in trainning time and optimization this means.\n",
    "\n",
    "We are using now 263 variables of the 550 availables, this means over a 52% dimension reduction with similar results as in the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the performance we will get forecasting energy in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_attr = Pipeline([\n",
    "      ('feature_selection', feature_selection.SelectKBest(feature_selection.f_regression,\n",
    "                                                          k=rf_attr_optuna.best_params['k'])),\n",
    "      ('regression', ensemble.RandomForestRegressor(\n",
    "          random_state=random_state,\n",
    "          min_samples_split=rf_attr_optuna.best_params['min_samples_split'],\n",
    "          criterion=rf_attr_optuna.best_params['criterion'],\n",
    "          max_depth=rf_attr_optuna.best_params['max_depth'],\n",
    "          n_estimators=rf_attr_optuna.best_params['n_estimators'],\n",
    "          max_features=rf_attr_optuna.best_params['max_features']\n",
    "      ))\n",
    "    ])\n",
    "rf_attr = rf_attr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attribute selection model performance:\n",
      "RMSE: 447.8903 (should be lower than the trivial predictor using the mean MSE: 689.8476)\n",
      "R square: 0.5785 (should be higher than the trivial predictor using the mean: R square 0.0000)\n"
     ]
    }
   ],
   "source": [
    "rf_attr_predict = rf_attr.predict(x_test)\n",
    "print('\\nAttribute selection model performance:')\n",
    "print('RMSE: {:.4f} (should be lower than the trivial predictor using the mean MSE: {:.4f})'.format(\n",
    "    math.sqrt(metrics.mean_squared_error(y_test, rf_attr_predict)),\n",
    "    math.sqrt(metrics.mean_squared_error(y_test, [y_test.mean() for i in range(len(y_test))]))))\n",
    "print('R square: {:.4f} (should be higher than the trivial predictor using the mean: R square {:.4f})'.format(\n",
    "    metrics.r2_score(y_test, rf_attr_predict),\n",
    "    metrics.r2_score(y_test, [y_test.mean() for i in range(len(y_test))])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Use only Sotavento attributes\n",
    "**Is it enough to use only the attributes for the actual Sotavento location? (13th location in the grid)**\n",
    "\n",
    "We will select only Sotavento attributes and use again random forest as in previous section to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_depth = 2\n",
    "max_max_depth = 32\n",
    "min_n_estimators = 50\n",
    "max_n_estimators = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2528, 22) (1299, 22) (2110, 22)\n"
     ]
    }
   ],
   "source": [
    "#Selecting sotavento attributes only\n",
    "sot_attr = []\n",
    "for attr in x_train.columns:\n",
    "    if int(attr.split('.')[-1]) == 13:\n",
    "        sot_attr.append(attr)\n",
    "\n",
    "x_train_sot = x_train[sot_attr]\n",
    "x_val_sot = x_val[sot_attr]\n",
    "x_test_sot = x_test[sot_attr]\n",
    "print(x_train_sot.shape,x_val_sot.shape,x_test_sot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-01-14 17:22:31,720]\u001b[0m A new study created in memory with name: no-name-15192830-873f-4389-bd91-e0a0bb1c0579\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:22:34,513]\u001b[0m Trial 0 finished with value: 625.99684692581 and parameters: {'min_samples_split': 0.5927898583368811, 'criterion': 'mae', 'max_depth': 4, 'n_estimators': 334, 'max_features': 0.05012023176667339}. Best is trial 0 with value: 625.99684692581.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:22:35,665]\u001b[0m Trial 1 finished with value: 464.69454593130445 and parameters: {'min_samples_split': 0.4969889299707142, 'criterion': 'mse', 'max_depth': 5, 'n_estimators': 363, 'max_features': 0.3651138018549163}. Best is trial 1 with value: 464.69454593130445.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:22:43,275]\u001b[0m Trial 2 finished with value: 464.40310675536 and parameters: {'min_samples_split': 0.169789038688811, 'criterion': 'mae', 'max_depth': 31, 'n_estimators': 203, 'max_features': 0.12474337025562236}. Best is trial 2 with value: 464.40310675536.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:22:43,680]\u001b[0m Trial 3 finished with value: 668.4039473777632 and parameters: {'min_samples_split': 0.9761751731852542, 'criterion': 'mse', 'max_depth': 17, 'n_estimators': 397, 'max_features': 0.4302450132367066}. Best is trial 2 with value: 464.40310675536.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:22:44,305]\u001b[0m Trial 4 finished with value: 719.6218998777837 and parameters: {'min_samples_split': 0.8220678689386206, 'criterion': 'mae', 'max_depth': 28, 'n_estimators': 337, 'max_features': 0.30833069949314745}. Best is trial 2 with value: 464.40310675536.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:22:44,480]\u001b[0m Trial 5 finished with value: 719.66086420174 and parameters: {'min_samples_split': 0.916522383858843, 'criterion': 'mae', 'max_depth': 18, 'n_estimators': 93, 'max_features': 0.21900262569250614}. Best is trial 2 with value: 464.40310675536.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:22:44,932]\u001b[0m Trial 6 finished with value: 719.6476845083547 and parameters: {'min_samples_split': 0.8832977758240164, 'criterion': 'mae', 'max_depth': 25, 'n_estimators': 249, 'max_features': 0.08385580941731736}. Best is trial 2 with value: 464.40310675536.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:22:45,736]\u001b[0m Trial 7 finished with value: 430.6081164202202 and parameters: {'min_samples_split': 0.17800363127173735, 'criterion': 'mse', 'max_depth': 13, 'n_estimators': 218, 'max_features': 0.18813625783035576}. Best is trial 7 with value: 430.6081164202202.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:22:49,696]\u001b[0m Trial 8 finished with value: 556.8286966255381 and parameters: {'min_samples_split': 0.5818053340663004, 'criterion': 'mae', 'max_depth': 29, 'n_estimators': 132, 'max_features': 0.26005923052722485}. Best is trial 7 with value: 430.6081164202202.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:22:50,342]\u001b[0m Trial 9 finished with value: 719.6065891407362 and parameters: {'min_samples_split': 0.9166028756989812, 'criterion': 'mae', 'max_depth': 26, 'n_estimators': 350, 'max_features': 0.0036666804638394446}. Best is trial 7 with value: 430.6081164202202.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:22:53,222]\u001b[0m Trial 10 finished with value: 383.9188052119373 and parameters: {'min_samples_split': 0.006689478683302136, 'criterion': 'mse', 'max_depth': 9, 'n_estimators': 239, 'max_features': 0.5924410664035571}. Best is trial 10 with value: 383.9188052119373.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:22:56,338]\u001b[0m Trial 11 finished with value: 382.59872554041704 and parameters: {'min_samples_split': 0.013142083191492765, 'criterion': 'mse', 'max_depth': 10, 'n_estimators': 239, 'max_features': 0.5913896733849716}. Best is trial 11 with value: 382.59872554041704.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:22:59,521]\u001b[0m Trial 12 finished with value: 382.63503679375106 and parameters: {'min_samples_split': 0.02017390177428556, 'criterion': 'mse', 'max_depth': 10, 'n_estimators': 249, 'max_features': 0.5837480952563069}. Best is trial 11 with value: 382.59872554041704.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:23:02,558]\u001b[0m Trial 13 finished with value: 383.4462821996686 and parameters: {'min_samples_split': 0.031419912658474214, 'criterion': 'mse', 'max_depth': 11, 'n_estimators': 289, 'max_features': 0.5307168046062054}. Best is trial 11 with value: 382.59872554041704.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:23:03,947]\u001b[0m Trial 14 finished with value: 411.58442466643044 and parameters: {'min_samples_split': 0.2872448744023493, 'criterion': 'mse', 'max_depth': 7, 'n_estimators': 165, 'max_features': 0.5997872228707556}. Best is trial 11 with value: 382.59872554041704.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:23:04,925]\u001b[0m Trial 15 finished with value: 481.5128580585649 and parameters: {'min_samples_split': 0.35718270340398817, 'criterion': 'mse', 'max_depth': 2, 'n_estimators': 285, 'max_features': 0.49240477231499924}. Best is trial 11 with value: 382.59872554041704.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:23:06,629]\u001b[0m Trial 16 finished with value: 408.44330884114373 and parameters: {'min_samples_split': 0.10637644795034118, 'criterion': 'mse', 'max_depth': 14, 'n_estimators': 183, 'max_features': 0.5224841553601824}. Best is trial 11 with value: 382.59872554041704.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:23:10,679]\u001b[0m Trial 17 finished with value: 378.36972489322824 and parameters: {'min_samples_split': 0.0009326810839291322, 'criterion': 'mse', 'max_depth': 21, 'n_estimators': 290, 'max_features': 0.42268772785940734}. Best is trial 17 with value: 378.36972489322824.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:23:12,262]\u001b[0m Trial 18 finished with value: 420.5167885589532 and parameters: {'min_samples_split': 0.31926126547868217, 'criterion': 'mse', 'max_depth': 21, 'n_estimators': 303, 'max_features': 0.41893347172658624}. Best is trial 17 with value: 378.36972489322824.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 17:23:12,692]\u001b[0m Trial 19 finished with value: 416.4657241094535 and parameters: {'min_samples_split': 0.1238476360825328, 'criterion': 'mse', 'max_depth': 21, 'n_estimators': 56, 'max_features': 0.4442599593678897}. Best is trial 17 with value: 378.36972489322824.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random_state)\n",
    "def random_forest_sot_objective(trial):\n",
    "    min_samples_split = trial.suggest_uniform('min_samples_split', 0+sys.float_info.min, 1)\n",
    "    criterion = trial.suggest_categorical('criterion', ['mse','mae'])\n",
    "    max_depth = trial.suggest_int('max_depth', min_max_depth, max_max_depth)\n",
    "    n_estimators = trial.suggest_int('n_estimators', min_n_estimators, max_n_estimators)\n",
    "    max_features = trial.suggest_uniform('max_features', 0+sys.float_info.min, 0.6)\n",
    "\n",
    "    clf = ensemble.RandomForestRegressor(\n",
    "        random_state=random_state,\n",
    "        min_samples_split=min_samples_split,\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        max_features=max_features\n",
    "        )\n",
    "\n",
    "    clf = clf.fit(x_train_sot, y_train)\n",
    "    y_val_pred = clf.predict(x_val_sot)\n",
    "    return math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "rf_sot_optuna = optuna.create_study(direction='minimize')\n",
    "start_time = time.time()\n",
    "rf_sot_optuna.optimize(random_forest_sot_objective, n_trials=budget, n_jobs=n_jobs)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 0.0009326810839291322, 'criterion': 'mse', 'max_depth': 21, 'n_estimators': 290, 'max_features': 0.42268772785940734} 378.36972489322824\n"
     ]
    }
   ],
   "source": [
    "print(rf_sot_optuna.best_params, rf_sot_optuna.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use a model using only the Sotavento attributes we can achive similar results as in the first section. Again, the RMSE is not as low as we have already obtained, but now we are using 13 attributes instead of the 550 availables (97% dimension reduction) so the tradeof between performance and execution time does really pay off.\n",
    "\n",
    "We will now estimate the performance in predicting energy generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_sot = ensemble.RandomForestRegressor(\n",
    "          random_state=random_state,\n",
    "          min_samples_split=rf_sot_optuna.best_params['min_samples_split'],\n",
    "          criterion=rf_sot_optuna.best_params['criterion'],\n",
    "          max_depth=rf_sot_optuna.best_params['max_depth'],\n",
    "          n_estimators=rf_sot_optuna.best_params['n_estimators'],\n",
    "          max_features=rf_sot_optuna.best_params['max_features']\n",
    "      )\n",
    "rf_sot = rf_sot.fit(x_train_sot, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attribute selection model performance:\n",
      "RMSE: 387.4636 (should be lower than the trivial predictor using the mean MSE: 689.8476)\n",
      "R square: 0.6845 (should be higher than the trivial predictor using the mean: R square 0.0000)\n"
     ]
    }
   ],
   "source": [
    "rf_sot_predict = rf_sot.predict(x_test_sot)\n",
    "print('\\nAttribute selection model performance:')\n",
    "print('RMSE: {:.4f} (should be lower than the trivial predictor using the mean MSE: {:.4f})'.format(\n",
    "    math.sqrt(metrics.mean_squared_error(y_test, rf_sot_predict)),\n",
    "    math.sqrt(metrics.mean_squared_error(y_test, [y_test.mean() for i in range(len(y_test))]))))\n",
    "print('R square: {:.4f} (should be higher than the trivial predictor using the mean: R square {:.4f})'.format(\n",
    "    metrics.r2_score(y_test, rf_sot_predict),\n",
    "    metrics.r2_score(y_test, [y_test.mean() for i in range(len(y_test))])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we expect to get a better performance with this Sotavento model than with the one used before attribute selection and it is really close to random forest and gradient boosting with all attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusions\n",
    "\n",
    "We have obtained really good models to forecast energy generation with precision even though we artificially remove data from the dataset. First, we had to deal with this missing data. As the matrix we are dealing with is really big and due to our computing power restrictions, we could not really use advanced techniques to imput missing data as an iterative imputer or a KNN imputer. Therefore, we just put the sample median in the empty spots we generated.\n",
    "\n",
    "Then, we have trained KNN, random forest and gradient boosting models. We have used an advanced hyper-parameter tunning optimizer as optuna and also xgboost library to get a better approach than with sklearn which models predict worst and are slower for trainning in this case. Again, we have to deal with the problem of computation time and because of computing power restirctions we cannot set a really high budget or times for trainning will be  really high for the assignment. After all, we obtained two good models for random forest and gradient boosting (xgboost) that could be used to predict energy generation.\n",
    "\n",
    "Lastly, we have tried to do some attribute selection. We first try it by using pure computation and with Pipelines to first select the best attributes to predict and then train a random forest model with these specifications. The results were slightly worse than trainning with all the attributes, but satisfactory taking into account the dimension reduction. Then, we use only the Sotavento data that we have and again get an interesting model, not as good as the one using all the parameters, but another time sufficient if we think in the dimension reduction.\n",
    "\n",
    "As a final conclusion, this assignment has made us undertand the importance of planning when trainning really big models as a lot of time could be wasted. We get the impression that we could have achieved quite better results with more powerful computers that will have helped us with deeper hyper-parameter tunning, but we are satisfied with the results, the knowledge and conclusion we have obtained during the assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
