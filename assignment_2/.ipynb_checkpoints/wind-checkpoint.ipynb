{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind prediction - Second assignment\n",
    "\n",
    "## Authors\n",
    "\n",
    "David Moreno Maldonado 100441714     \n",
    "Inés Fernández Campos 100443936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/roni/Desktop/master/2nd quarter/big data intelligence/assignments/assignment_2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import some libraries\n",
    "import os\n",
    "import numpy as np              \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn import preprocessing, impute, model_selection, metrics, neighbors, ensemble, feature_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "import optuna\n",
    "import optuna.visualization as ov\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN PARAMETERS FOR THE ASSIGNMENT\n",
    "budget = 20\n",
    "random_state = 3\n",
    "verbose = 0\n",
    "n_jobs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"wind_pickle\" file contains data in a binary format called \"Pickle\". Pickle data loads faster than text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('wind_pickle.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the attributes in the dataset. Very important, the output attribute (i.e. the value to be predicted, **energy**, is the first attribute). **Steps** represents the hours in advance of the forecast. We will not use this variable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5937, 556)\n"
     ]
    }
   ],
   "source": [
    "# The dataset contains 5937 instances and 556 attributes (including the outcome to be predicted)\n",
    "print(data.shape)\n",
    "#data.columns.values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-1 for training, 0 for validation, 1 for testing\n",
    "year_to_part = {\n",
    "    2005: -1,\n",
    "    2006: -1,\n",
    "    2007: 0,\n",
    "    2008: 0, \n",
    "    2009: 1,\n",
    "    2010: 1\n",
    "}\n",
    "data['partition'] = data['year'].apply(lambda x: year_to_part[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now remove the columns that cannot be used for training the models from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps, month, day, hour, year should be removed, they cannot be used for training the models\n",
    "to_remove = ['steps', 'month', 'year', 'day', 'hour']\n",
    "for m in to_remove: data = data.drop(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets put 163861 missing values \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import randint\n",
    "\n",
    "# we add na values at random\n",
    "my_NIA = 100443936 + 100441714\n",
    "np.random.seed(my_NIA)\n",
    "\n",
    "how_many_nas = round(data.shape[0]*data.shape[1]*0.05)\n",
    "print('Lets put '+str(how_many_nas)+' missing values \\n')\n",
    "x_locations = randint(0, data.shape[0], size=how_many_nas)\n",
    "y_locations = randint(1, data.shape[1]-2, size=how_many_nas)\n",
    "\n",
    "for i in range(len(x_locations)):\n",
    "    data.iat[x_locations[i], y_locations[i]] = np.nan\n",
    "    \n",
    "data.to_pickle('wind_pickle_with_nan.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on, the file wind_pickle_with_nan should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5937, 552)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('wind_pickle_with_nan.pickle')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have randomly inputed missing values throughout our data, prior to creating our models we must impute the missing values (except in the response). In the following cell we have implemented and iterative imputer two different ways (through *knnImputer* and *IterativeImputer*) and finally a simple imputer which is the one we have left uncommented simply because, although the first two are more complex and impute values using the entire set of available feature dimensions to estimate the missing values, they take far too long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().values.any())\n",
    "input_cols = data.columns.difference(['energy', 'partition'])\n",
    "x = data[input_cols]\n",
    "\n",
    "#Iterative imputer (takes too long)\n",
    "'''iter_imp = impute.IterativeImputer(random_state=random_state, \n",
    "                                   initial_strategy='median', \n",
    "                                   max_iter=3,\n",
    "                                   verbose=verbose)\n",
    "no_nan = iter_imp.fit_transform(x)'''\n",
    "\n",
    "#KNN imputer(takes too long)\n",
    "'''knn_imp = impute.KNNImputer(weights='distance')\n",
    "no_nan = knn_imp.fit_transform(x)'''\n",
    "\n",
    "#Simple imputer\n",
    "simp_imp = impute.SimpleImputer(strategy='median',\n",
    "                               verbose=2)\n",
    "no_nan = simp_imp.fit_transform(x)\n",
    "\n",
    "data[input_cols] = pd.DataFrame(data=no_nan)\n",
    "print(data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Lastly we scale the data so that it is all within the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(data[input_cols]) \n",
    "data[input_cols] = scaler.transform(data[input_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "We are going to use train/test for model evaluation (outer) and train/validation for hyperparameter tuning (inner), as follows:     \n",
    "1. Train partition: the first two years of data. Given that there are 6 years worth of data, we will use the first 2/6 of the instances for training.     \n",
    "2. Validation partition: the second two years of data. \n",
    "3. Test partition: the remaining data    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-1 for training, 0 for validation, 1 for testing\n",
    "test = data[data['partition'] == 1]\n",
    "train = data[data['partition'] == -1]\n",
    "val = data[data['partition'] == 0]\n",
    "\n",
    "del test['partition']\n",
    "del train['partition']\n",
    "\n",
    "y_test = test['energy']\n",
    "x_test = test[test.columns.difference(['energy'])]\n",
    "\n",
    "y_train = train['energy']\n",
    "x_train = train[train.columns.difference(['energy'])]\n",
    "\n",
    "\n",
    "y_val = val['energy']\n",
    "x_val = val[train.columns.difference(['energy'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MODEL SELECTION AND HYPER-PARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframes with all the information of each model\n",
    "summary = {\n",
    "    'knn': pd.DataFrame(columns=['Time (sec)', 'Score (RMSE)', 'N. neighbors', 'Weights', 'P']),\n",
    "    'random_forest': pd.DataFrame(columns=['Time (sec)', 'Score (RMSE)', 'Min. samples split', 'Criterion', 'Max. depth', 'N. estimators','Max. features']),\n",
    "    'gradient_boosting': pd.DataFrame(columns=['Time (sec)', 'Score (RMSE)'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Default hyper-parameters\n",
    "\n",
    "Here we train KNN with the default hyper-parameters, so the number of neighbors used will be 5 and the power parameter for the Minkowski metric is set to 2, so KNN will be using euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_state)\n",
    "knn_default = neighbors.KNeighborsRegressor()\n",
    "\n",
    "start_time = time.time()\n",
    "knn_default = knn_default.fit(x_train, y_train)\n",
    "y_val_pred = knn_default.predict(x_val)\n",
    "score = math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "end_time = time.time()\n",
    "\n",
    "summary['knn'] = summary['knn'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': score, \n",
    "    'N. neighbors': 5, \n",
    "    'Weights': 'uniform', \n",
    "    'P': 2\n",
    "    }, \n",
    "    name='default'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Hyper-parameter tunning (OPTUNA)\n",
    "\n",
    "In this subsection however, we are going to tune the hyper-parameters using Optuna.  \n",
    "To do so, we create and objective function that will test a set of hyper-parameters for the model and evaluate the model's performance given those hyper-parameters and return that score. Within this objective function, we define three hyper-parameters to tune: \n",
    "- number of neighbors: taking any integer value in [1, 16].\n",
    "- weights: weight function to be used in prediction can either be uniform weights or points can be distance weighted (closer implies more influence).\n",
    "- p: power parametric for the Minkowski metric in order to choose between euclidean distance or manhattan distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_n_neigbors = 1\n",
    "max_n_neigbors = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-01-14 10:11:50,254]\u001b[0m A new study created in memory with name: no-name-e105da23-6593-43a3-901e-128c7b8ff3a9\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:11:56,859]\u001b[0m Trial 0 finished with value: 428.79923039544184 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'p': 1}. Best is trial 0 with value: 428.79923039544184.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:12:02,180]\u001b[0m Trial 1 finished with value: 427.85882725553654 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'p': 1}. Best is trial 1 with value: 427.85882725553654.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:12:07,368]\u001b[0m Trial 2 finished with value: 427.1897361344681 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'p': 1}. Best is trial 2 with value: 427.1897361344681.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:12:12,288]\u001b[0m Trial 3 finished with value: 495.6488893531908 and parameters: {'n_neighbors': 2, 'weights': 'uniform', 'p': 1}. Best is trial 2 with value: 427.1897361344681.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:12:16,805]\u001b[0m Trial 4 finished with value: 437.74273073498597 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'p': 2}. Best is trial 2 with value: 427.1897361344681.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:12:23,186]\u001b[0m Trial 5 finished with value: 425.19635407015437 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'p': 1}. Best is trial 5 with value: 425.19635407015437.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:12:31,894]\u001b[0m Trial 6 finished with value: 427.41973254231516 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'p': 1}. Best is trial 5 with value: 425.19635407015437.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:12:38,688]\u001b[0m Trial 7 finished with value: 425.8532193925109 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'p': 1}. Best is trial 5 with value: 425.19635407015437.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:12:44,388]\u001b[0m Trial 8 finished with value: 496.3786220738082 and parameters: {'n_neighbors': 2, 'weights': 'distance', 'p': 1}. Best is trial 5 with value: 425.19635407015437.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:12:50,504]\u001b[0m Trial 9 finished with value: 429.05463080771233 and parameters: {'n_neighbors': 16, 'weights': 'uniform', 'p': 1}. Best is trial 5 with value: 425.19635407015437.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:12:55,892]\u001b[0m Trial 10 finished with value: 446.53181088677667 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'p': 2}. Best is trial 5 with value: 425.19635407015437.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:13:01,970]\u001b[0m Trial 11 finished with value: 424.95488026361073 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'p': 1}. Best is trial 11 with value: 424.95488026361073.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:13:07,984]\u001b[0m Trial 12 finished with value: 424.97500012000376 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'p': 1}. Best is trial 11 with value: 424.95488026361073.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:13:13,154]\u001b[0m Trial 13 finished with value: 437.00804583495204 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'p': 2}. Best is trial 11 with value: 424.95488026361073.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:13:18,697]\u001b[0m Trial 14 finished with value: 441.1599884529427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'p': 1}. Best is trial 11 with value: 424.95488026361073.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:13:25,771]\u001b[0m Trial 15 finished with value: 424.95488026361073 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'p': 1}. Best is trial 11 with value: 424.95488026361073.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:13:31,710]\u001b[0m Trial 16 finished with value: 426.6970286295103 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'p': 1}. Best is trial 11 with value: 424.95488026361073.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:13:36,767]\u001b[0m Trial 17 finished with value: 440.6302777837268 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'p': 2}. Best is trial 11 with value: 424.95488026361073.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:13:42,488]\u001b[0m Trial 18 finished with value: 451.6020085952498 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'p': 1}. Best is trial 11 with value: 424.95488026361073.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:13:48,121]\u001b[0m Trial 19 finished with value: 424.95488026361073 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'p': 1}. Best is trial 11 with value: 424.95488026361073.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random_state)\n",
    "def knn_objective(trial):\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', min_n_neigbors, max_n_neigbors)\n",
    "    weights = trial.suggest_categorical('weights', ['uniform','distance'])\n",
    "    p = trial.suggest_categorical('p', [1, 2])\n",
    "\n",
    "    clf = neighbors.KNeighborsRegressor(\n",
    "        n_neighbors=n_neighbors,\n",
    "        weights=weights,\n",
    "        p=p)\n",
    "    \n",
    "    clf = clf.fit(x_train, y_train)\n",
    "    y_val_pred = clf.predict(x_val)\n",
    "    return math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "knn_optuna = optuna.create_study(direction='minimize')\n",
    "start_time = time.time()\n",
    "knn_optuna.optimize(knn_objective, n_trials=budget)\n",
    "end_time = time.time()\n",
    "\n",
    "summary['knn'] = summary['knn'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': knn_optuna.best_value, \n",
    "    'N. neighbors': knn_optuna.best_params['n_neighbors'], \n",
    "    'Weights': knn_optuna.best_params['weights'], \n",
    "    'P': knn_optuna.best_params['p']\n",
    "    }, \n",
    "    name='optuna'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Default hyper-parameters\n",
    "\n",
    "The following cell trains a random forest ensemble with default parameters: 100 estimators, mse criteiron and 2 samples minimum to continue splitting a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_state)\n",
    "rf_default = ensemble.RandomForestRegressor(random_state=random_state, verbose=verbose, n_jobs=n_jobs)\n",
    "\n",
    "start_time = time.time()\n",
    "rf_default = rf_default.fit(x_train, y_train)\n",
    "y_val_pred = rf_default.predict(x_val)\n",
    "score =  math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "end_time = time.time()\n",
    "\n",
    "summary['random_forest'] = summary['random_forest'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': score,\n",
    "    'Min. samples split': 2, \n",
    "    'Criterion': 'mse', \n",
    "    'Max. depth': 'None',\n",
    "    'N. estimators': 100,\n",
    "    'Max. features': 1\n",
    "    },\n",
    "    name='default'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Hyper-parameter tunning (OPTUNA)\n",
    "\n",
    "Once again, instead of settling for the default hyper-parameters we tune them using Optuna. In this case more hyper-parameters are tuned but the procedure is similar if not pratically the same to how we tuned the KNNRegressor using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_depth = 2\n",
    "max_max_depth = 50\n",
    "min_n_estimators = 50\n",
    "max_n_estimators = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-01-13 14:40:46,203]\u001b[0m A new study created in memory with name: no-name-2663b009-5998-4065-8d0e-10ee2f47a415\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:40:48,116]\u001b[0m Trial 0 finished with value: 400.9598210339549 and parameters: {'min_samples_split': 0.2585716150647326, 'criterion': 'mse', 'max_depth': 21, 'n_estimators': 119, 'max_features': 'sqrt'}. Best is trial 0 with value: 400.9598210339549.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:40:48,275]\u001b[0m Trial 1 finished with value: 668.3836578998909 and parameters: {'min_samples_split': 0.7275958768930648, 'criterion': 'mse', 'max_depth': 19, 'n_estimators': 125, 'max_features': 'sqrt'}. Best is trial 0 with value: 400.9598210339549.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:40:48,379]\u001b[0m Trial 2 finished with value: 668.4502497483671 and parameters: {'min_samples_split': 0.9734866223139015, 'criterion': 'mse', 'max_depth': 19, 'n_estimators': 83, 'max_features': 'sqrt'}. Best is trial 0 with value: 400.9598210339549.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:40:48,521]\u001b[0m Trial 3 finished with value: 668.3890217234632 and parameters: {'min_samples_split': 0.9904091845943086, 'criterion': 'mse', 'max_depth': 12, 'n_estimators': 122, 'max_features': 'log2'}. Best is trial 0 with value: 400.9598210339549.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:41:19,220]\u001b[0m Trial 4 finished with value: 424.4919801908068 and parameters: {'min_samples_split': 0.2608742841494963, 'criterion': 'mae', 'max_depth': 44, 'n_estimators': 198, 'max_features': 'log2'}. Best is trial 0 with value: 400.9598210339549.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:41:21,039]\u001b[0m Trial 5 finished with value: 401.3502545053183 and parameters: {'min_samples_split': 0.2735220385298276, 'criterion': 'mse', 'max_depth': 25, 'n_estimators': 125, 'max_features': 'sqrt'}. Best is trial 0 with value: 400.9598210339549.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:42:30,503]\u001b[0m Trial 6 finished with value: 412.5364997638835 and parameters: {'min_samples_split': 0.2726886151779092, 'criterion': 'mae', 'max_depth': 19, 'n_estimators': 168, 'max_features': 'sqrt'}. Best is trial 0 with value: 400.9598210339549.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:43:17,957]\u001b[0m Trial 7 finished with value: 394.1882243101476 and parameters: {'min_samples_split': 0.19324574838516728, 'criterion': 'mse', 'max_depth': 25, 'n_estimators': 124, 'max_features': 'auto'}. Best is trial 7 with value: 394.1882243101476.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:43:18,264]\u001b[0m Trial 8 finished with value: 618.354981330637 and parameters: {'min_samples_split': 0.6373094267217242, 'criterion': 'mse', 'max_depth': 7, 'n_estimators': 150, 'max_features': 'sqrt'}. Best is trial 7 with value: 394.1882243101476.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:43:18,582]\u001b[0m Trial 9 finished with value: 719.4164964060726 and parameters: {'min_samples_split': 0.7003662398026363, 'criterion': 'mae', 'max_depth': 49, 'n_estimators': 138, 'max_features': 'auto'}. Best is trial 7 with value: 394.1882243101476.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:43:57,283]\u001b[0m Trial 10 finished with value: 380.3488406352341 and parameters: {'min_samples_split': 0.02490355296388508, 'criterion': 'mse', 'max_depth': 37, 'n_estimators': 63, 'max_features': 'auto'}. Best is trial 10 with value: 380.3488406352341.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:44:43,805]\u001b[0m Trial 11 finished with value: 377.26906792381976 and parameters: {'min_samples_split': 0.00363889789936404, 'criterion': 'mse', 'max_depth': 37, 'n_estimators': 59, 'max_features': 'auto'}. Best is trial 11 with value: 377.26906792381976.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:45:19,882]\u001b[0m Trial 12 finished with value: 377.5159617676009 and parameters: {'min_samples_split': 0.01066118844432426, 'criterion': 'mse', 'max_depth': 37, 'n_estimators': 53, 'max_features': 'auto'}. Best is trial 11 with value: 377.26906792381976.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:45:55,865]\u001b[0m Trial 13 finished with value: 376.9700103522176 and parameters: {'min_samples_split': 0.005353556937554937, 'criterion': 'mse', 'max_depth': 36, 'n_estimators': 50, 'max_features': 'auto'}. Best is trial 13 with value: 376.9700103522176.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:46:33,163]\u001b[0m Trial 14 finished with value: 394.20574848927464 and parameters: {'min_samples_split': 0.1110851413902727, 'criterion': 'mse', 'max_depth': 34, 'n_estimators': 86, 'max_features': 'auto'}. Best is trial 13 with value: 376.9700103522176.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:46:47,008]\u001b[0m Trial 15 finished with value: 420.15132134218635 and parameters: {'min_samples_split': 0.456175516377316, 'criterion': 'mse', 'max_depth': 32, 'n_estimators': 76, 'max_features': 'auto'}. Best is trial 13 with value: 376.9700103522176.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:53:11,102]\u001b[0m Trial 16 finished with value: 431.99886387099576 and parameters: {'min_samples_split': 0.41452703634839544, 'criterion': 'mae', 'max_depth': 43, 'n_estimators': 53, 'max_features': 'auto'}. Best is trial 13 with value: 376.9700103522176.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:54:12,879]\u001b[0m Trial 17 finished with value: 392.59803315665596 and parameters: {'min_samples_split': 0.09157790021954865, 'criterion': 'mse', 'max_depth': 49, 'n_estimators': 100, 'max_features': 'auto'}. Best is trial 13 with value: 376.9700103522176.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:54:14,452]\u001b[0m Trial 18 finished with value: 375.827967578847 and parameters: {'min_samples_split': 0.005758582581728201, 'criterion': 'mse', 'max_depth': 31, 'n_estimators': 65, 'max_features': 'log2'}. Best is trial 18 with value: 375.827967578847.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 14:54:15,769]\u001b[0m Trial 19 finished with value: 410.8563146030943 and parameters: {'min_samples_split': 0.13721976657959345, 'criterion': 'mse', 'max_depth': 30, 'n_estimators': 102, 'max_features': 'log2'}. Best is trial 18 with value: 375.827967578847.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random_state)\n",
    "def random_forest_objective(trial):\n",
    "    min_samples_split = trial.suggest_uniform('min_samples_split', 0+sys.float_info.min, 1)\n",
    "    criterion = trial.suggest_categorical('criterion', ['mse','mae'])\n",
    "    max_depth = trial.suggest_int('max_depth', min_max_depth, max_max_depth)\n",
    "    n_estimators = trial.suggest_int('n_estimators', min_n_estimators, max_n_estimators)\n",
    "    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2'])\n",
    "\n",
    "    clf = ensemble.RandomForestRegressor(\n",
    "        random_state=random_state,\n",
    "        min_samples_split=min_samples_split,\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        max_features=max_features\n",
    "        )\n",
    "    clf = clf.fit(x_train, y_train)\n",
    "    y_val_pred = clf.predict(x_val)\n",
    "    return math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "rf_optuna = optuna.create_study(direction='minimize')\n",
    "start_time = time.time()\n",
    "rf_optuna.optimize(random_forest_objective, n_trials=budget, n_jobs=n_jobs)\n",
    "end_time = time.time()\n",
    "\n",
    "summary['random_forest'] = summary['random_forest'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': rf_optuna.best_value,\n",
    "    'Min. samples split': rf_optuna.best_params['min_samples_split'], \n",
    "    'Criterion': rf_optuna.best_params['criterion'], \n",
    "    'Max. depth': rf_optuna.best_params['max_depth'],\n",
    "    'N. estimators': rf_optuna.best_params['n_estimators'],\n",
    "    'Max. features': rf_optuna.best_params['max_features']\n",
    "    },\n",
    "    name='optuna'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Gradient Boosting\n",
    "\n",
    "In this section we seek to apply Gradient Boosting. Boosting merely wants to boost weak models (as are our trees) through ensembles by sequentially adding a new model to that ensemble with the idea that every model added to the ensemble might do better than the last.\n",
    "\n",
    "Gradient Boosting takes this idea of Boosting and applies it through Gradient Descent. Basically, every added model will approximate the distance between the ensembles output at that iteration and the actual output we want to get. And by adding new models to our ensembles we expect that difference of outputs to decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Default hyper-parameters\n",
    "\n",
    "Here we first implement Gradient Boosting using the scikit-learn library with default hyper-parameters. \n",
    "\n",
    "Note how, since we are not only working with regression trees but gradient descent, apart from hyper-parameters for the trees we now find hyper-parameters like *learning rate* that are used to determine the steplegnth in the descent direction towards the optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation using sklearn\n",
    "np.random.seed(random_state)\n",
    "gb_sk_def = ensemble.GradientBoostingRegressor(random_state=random_state, verbose=verbose)\n",
    "\n",
    "start_time = time.time()\n",
    "gb_sk_def = gb_sk_def.fit(x_train, y_train)\n",
    "y_val_pred = gb_sk_def.predict(x_val)\n",
    "score =  math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "end_time = time.time()\n",
    "\n",
    "summary['gradient_boosting'] = summary['gradient_boosting'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': score,\n",
    "    'Learning rate': 0.1,\n",
    "    'N. estimators': 100,\n",
    "    'Criterion': 'friedman_mse', \n",
    "    'Min. samples split': 2, \n",
    "    'Min. samples leaf': 1,\n",
    "    'Max. depth': 3,\n",
    "    'Max. leaf nodes': 'None'\n",
    "    },\n",
    "    name='default'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also implemented Gradient Boosting using the XGBoost library.\n",
    " \n",
    "The implementation is pratically the same, only having to transform or cast the input training and test matrices to the libraries matrices.\n",
    "Regarding the hyper-parameters, some that might be present in scikit are not in xgb or vice-versa but let us implement Gradient Descent this way and later on evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation using xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "gb_xgb_def = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "start_time = time.time()\n",
    "gb_xgb_def = gb_xgb_def.fit(x_train, y_train)\n",
    "y_val_pred = gb_xgb_def.predict(x_val)\n",
    "score = math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "end_time = time.time()\n",
    "\n",
    "summary['gradient_boosting'] = summary['gradient_boosting'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': score,\n",
    "    'Learning rate': 0.3,\n",
    "    'Max. depth': 6,\n",
    "    'Max. leaf nodes': 0,\n",
    "    'Gamma (min_split_loss)': 0,\n",
    "    'Lambda': 1,\n",
    "    'Alpha': 0,\n",
    "    'N. estimators': gb_xgb_def.get_params()['n_estimators']\n",
    "    },\n",
    "    name='default_xgboost'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Hyper-parameter tunning\n",
    "\n",
    "As we did in the last subsection we will now implement Gradient Boosting for both scikit-learn and XGBoost tuning their hyper-parameters through Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_leaf_nodes = 2\n",
    "max_max_leaf_nodes = 20\n",
    "min_min_samples_leaf = 1\n",
    "max_min_samples_leaf = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn the execution took quite a while which is why we created a variable *short* to control how many hyper-parameters to train. However, the outputs execution you see below corresponds to training all hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-01-13 17:33:44,119]\u001b[0m A new study created in memory with name: no-name-85840a61-4374-4c5d-87c8-f6402e3d33fa\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:34:20,446]\u001b[0m Trial 0 finished with value: 426.3910818511826 and parameters: {'learning_rate': 0.9437204360706714, 'n_estimators': 179, 'min_samples_split': 0.9904165055278342, 'max_depth': 4, 'criterion': 'friedman_mse', 'min_samples_leaf': 5, 'max_leaf_nodes': 5}. Best is trial 0 with value: 426.3910818511826.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:36:14,804]\u001b[0m Trial 1 finished with value: 451.73522494764103 and parameters: {'learning_rate': 0.8426704875565854, 'n_estimators': 174, 'min_samples_split': 0.6297387035925248, 'max_depth': 6, 'criterion': 'friedman_mse', 'min_samples_leaf': 10, 'max_leaf_nodes': 15}. Best is trial 0 with value: 426.3910818511826.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:36:52,655]\u001b[0m Trial 2 finished with value: 471.6245212069269 and parameters: {'learning_rate': 0.9957578627484363, 'n_estimators': 96, 'min_samples_split': 0.8946037659207297, 'max_depth': 9, 'criterion': 'friedman_mse', 'min_samples_leaf': 4, 'max_leaf_nodes': 11}. Best is trial 0 with value: 426.3910818511826.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:37:09,263]\u001b[0m Trial 3 finished with value: 406.67011756131495 and parameters: {'learning_rate': 0.8371150444242546, 'n_estimators': 52, 'min_samples_split': 0.49078633378092584, 'max_depth': 7, 'criterion': 'mse', 'min_samples_leaf': 9, 'max_leaf_nodes': 2}. Best is trial 3 with value: 406.67011756131495.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:38:23,186]\u001b[0m Trial 4 finished with value: 378.534765140851 and parameters: {'learning_rate': 0.12904545476672757, 'n_estimators': 115, 'min_samples_split': 0.5949070997701705, 'max_depth': 5, 'criterion': 'friedman_mse', 'min_samples_leaf': 7, 'max_leaf_nodes': 5}. Best is trial 4 with value: 378.534765140851.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:40:03,616]\u001b[0m Trial 5 finished with value: 384.3678785967688 and parameters: {'learning_rate': 0.15727218364405904, 'n_estimators': 84, 'min_samples_split': 0.31692872611290845, 'max_depth': 9, 'criterion': 'friedman_mse', 'min_samples_leaf': 6, 'max_leaf_nodes': 12}. Best is trial 4 with value: 378.534765140851.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:41:47,454]\u001b[0m Trial 6 finished with value: 443.96440563773274 and parameters: {'learning_rate': 0.7161201918079666, 'n_estimators': 199, 'min_samples_split': 0.7911401667685287, 'max_depth': 8, 'criterion': 'mse', 'min_samples_leaf': 8, 'max_leaf_nodes': 17}. Best is trial 4 with value: 378.534765140851.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:43:36,205]\u001b[0m Trial 7 finished with value: 388.54385777534395 and parameters: {'learning_rate': 0.26800234196295825, 'n_estimators': 175, 'min_samples_split': 0.5995905355607194, 'max_depth': 5, 'criterion': 'friedman_mse', 'min_samples_leaf': 9, 'max_leaf_nodes': 17}. Best is trial 4 with value: 378.534765140851.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:44:25,465]\u001b[0m Trial 8 finished with value: 434.0440837996413 and parameters: {'learning_rate': 0.6607043829976477, 'n_estimators': 105, 'min_samples_split': 0.3682658581586651, 'max_depth': 3, 'criterion': 'mse', 'min_samples_leaf': 3, 'max_leaf_nodes': 5}. Best is trial 4 with value: 378.534765140851.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:46:20,729]\u001b[0m Trial 9 finished with value: 448.4228731582385 and parameters: {'learning_rate': 0.6605989830661771, 'n_estimators': 184, 'min_samples_split': 0.7831395508322547, 'max_depth': 15, 'criterion': 'friedman_mse', 'min_samples_leaf': 5, 'max_leaf_nodes': 17}. Best is trial 4 with value: 378.534765140851.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:48:14,210]\u001b[0m Trial 10 finished with value: 416.03446387762983 and parameters: {'learning_rate': 0.009670080431897835, 'n_estimators': 146, 'min_samples_split': 0.08662349610426745, 'max_depth': 13, 'criterion': 'friedman_mse', 'min_samples_leaf': 1, 'max_leaf_nodes': 7}. Best is trial 4 with value: 378.534765140851.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:49:19,749]\u001b[0m Trial 11 finished with value: 373.5031534726167 and parameters: {'learning_rate': 0.13012147162039495, 'n_estimators': 60, 'min_samples_split': 0.20892275826444368, 'max_depth': 10, 'criterion': 'friedman_mse', 'min_samples_leaf': 7, 'max_leaf_nodes': 9}. Best is trial 11 with value: 373.5031534726167.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:49:53,641]\u001b[0m Trial 12 finished with value: 399.8686837073088 and parameters: {'learning_rate': 0.3966774989624777, 'n_estimators': 50, 'min_samples_split': 0.0018539423143096312, 'max_depth': 11, 'criterion': 'friedman_mse', 'min_samples_leaf': 7, 'max_leaf_nodes': 8}. Best is trial 11 with value: 373.5031534726167.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:50:37,099]\u001b[0m Trial 13 finished with value: 562.4780275233212 and parameters: {'learning_rate': 0.006461915651622754, 'n_estimators': 134, 'min_samples_split': 0.20423428218614853, 'max_depth': 2, 'criterion': 'friedman_mse', 'min_samples_leaf': 7, 'max_leaf_nodes': 2}. Best is trial 11 with value: 373.5031534726167.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:51:43,972]\u001b[0m Trial 14 finished with value: 385.43405456841555 and parameters: {'learning_rate': 0.2181808124497991, 'n_estimators': 69, 'min_samples_split': 0.44730177719027986, 'max_depth': 11, 'criterion': 'friedman_mse', 'min_samples_leaf': 7, 'max_leaf_nodes': 8}. Best is trial 11 with value: 373.5031534726167.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:54:50,359]\u001b[0m Trial 15 finished with value: 414.53970397004514 and parameters: {'learning_rate': 0.4289674628773087, 'n_estimators': 120, 'min_samples_split': 0.19256391043083487, 'max_depth': 12, 'criterion': 'friedman_mse', 'min_samples_leaf': 10, 'max_leaf_nodes': 13}. Best is trial 11 with value: 373.5031534726167.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:55:29,838]\u001b[0m Trial 16 finished with value: 380.55105478026474 and parameters: {'learning_rate': 0.1014538797315698, 'n_estimators': 67, 'min_samples_split': 0.6379738821787163, 'max_depth': 6, 'criterion': 'mse', 'min_samples_leaf': 8, 'max_leaf_nodes': 5}. Best is trial 11 with value: 373.5031534726167.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:58:44,953]\u001b[0m Trial 17 finished with value: 397.7850845967097 and parameters: {'learning_rate': 0.3158396517373767, 'n_estimators': 147, 'min_samples_split': 0.28173827210244645, 'max_depth': 10, 'criterion': 'friedman_mse', 'min_samples_leaf': 3, 'max_leaf_nodes': 9}. Best is trial 11 with value: 373.5031534726167.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 17:59:52,058]\u001b[0m Trial 18 finished with value: 387.82029485211586 and parameters: {'learning_rate': 0.06764203312622216, 'n_estimators': 107, 'min_samples_split': 0.030591261739652875, 'max_depth': 14, 'criterion': 'friedman_mse', 'min_samples_leaf': 6, 'max_leaf_nodes': 3}. Best is trial 11 with value: 373.5031534726167.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:00:24,140]\u001b[0m Trial 19 finished with value: 413.53145388683106 and parameters: {'learning_rate': 0.5275054264891156, 'n_estimators': 73, 'min_samples_split': 0.54910640137881, 'max_depth': 2, 'criterion': 'friedman_mse', 'min_samples_leaf': 8, 'max_leaf_nodes': 20}. Best is trial 11 with value: 373.5031534726167.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# hyperparam tuning for sklearn ensemble.GradientBoostingRegressor\n",
    "np.random.seed(random_state)\n",
    "\n",
    "def gradboosting_objective(trial):  \n",
    "    gb_sk_opt = None\n",
    "    short = False\n",
    "    \n",
    "    learning_rate = trial.suggest_uniform('learning_rate', 0+sys.float_info.min, 1)\n",
    "    n_estimators = trial.suggest_int('n_estimators', min_n_estimators, max_n_estimators)\n",
    "    min_samples_split = trial.suggest_uniform('min_samples_split', 0+sys.float_info.min, 1)\n",
    "    max_depth = trial.suggest_int('max_depth', min_max_depth, max_max_depth)\n",
    "        \n",
    "    if short == False: # it will take a long time to run \n",
    "        criterion = trial.suggest_categorical('criterion', ['mse','friedman_mse'])\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf',min_min_samples_leaf, max_min_samples_leaf)\n",
    "        max_leaf_nodes = trial.suggest_int('max_leaf_nodes', min_max_leaf_nodes, max_max_leaf_nodes)\n",
    "        \n",
    "        gb_sk_opt = ensemble.GradientBoostingRegressor(learning_rate=learning_rate, \n",
    "                                                   n_estimators=n_estimators,\n",
    "                                                   criterion=criterion,\n",
    "                                                   min_samples_split=min_samples_split,\n",
    "                                                   min_samples_leaf=min_samples_leaf,\n",
    "                                                   max_depth=max_depth,\n",
    "                                                   max_leaf_nodes=max_leaf_nodes,\n",
    "                                                   random_state=random_state,\n",
    "                                                   verbose=verbose)\n",
    "    else:  # will take less time        \n",
    "        gb_sk_opt = ensemble.GradientBoostingRegressor(learning_rate=learning_rate, \n",
    "                                                   n_estimators=n_estimators,\n",
    "                                                   min_samples_split=min_samples_split,\n",
    "                                                   max_depth=max_depth,\n",
    "                                                   random_state=random_state,\n",
    "                                                   verbose=verbose)\n",
    "        \n",
    "    gb_sk_opt = gb_sk_opt.fit(x_train, y_train)\n",
    "    y_val_pred = gb_sk_opt.predict(x_val)\n",
    "    \n",
    "    return math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "gb_optuna = optuna.create_study(direction='minimize')\n",
    "start_time = time.time()\n",
    "gb_optuna.optimize(gradboosting_objective, n_trials=budget)\n",
    "end_time = time.time()\n",
    "\n",
    "summary['gradient_boosting'] = summary['gradient_boosting'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': gb_optuna.best_value,\n",
    "    'Learning rate': gb_optuna.best_params['learning_rate'],\n",
    "    'N. estimators': gb_optuna.best_params['n_estimators'],\n",
    "    'Criterion': 'friedman_mse', \n",
    "    'Min. samples leaf': gb_optuna.best_params['min_samples_leaf'],\n",
    "    'Min. samples leaf': 1,\n",
    "    'Max. depth': gb_optuna.best_params['max_depth'],\n",
    "    'Max. leaf nodes': gb_optuna.best_params['max_leaf_nodes']\n",
    "    },\n",
    "    name='optuna_sklearn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning XGBRegressor with Optuna the execution takes less time so there was no need to take a similar approach to what we did above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-01-13 18:00:31,508]\u001b[0m A new study created in memory with name: no-name-665d982e-98ea-4bc4-b946-dd024ab898e8\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:00:43,128]\u001b[0m Trial 0 finished with value: 407.8463263806792 and parameters: {'eta': 0.01578049329785458, 'max_depth': 3, 'n_estimators': 138, 'gamma': 0.6971905601309071, 'lambda': 0.3811080738096944, 'alpha': 0.20120626381188714}. Best is trial 0 with value: 407.8463263806792.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:00:52,135]\u001b[0m Trial 1 finished with value: 385.58132285995805 and parameters: {'eta': 0.11756590681846768, 'max_depth': 2, 'n_estimators': 161, 'gamma': 0.1854233470219595, 'lambda': 0.06834635874698405, 'alpha': 0.4742807662973112}. Best is trial 1 with value: 385.58132285995805.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:01:04,613]\u001b[0m Trial 2 finished with value: 421.71825083991206 and parameters: {'eta': 0.4634993828570153, 'max_depth': 4, 'n_estimators': 92, 'gamma': 0.5211171901404208, 'lambda': 0.25973284204659014, 'alpha': 0.06487483813687336}. Best is trial 1 with value: 385.58132285995805.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:01:16,513]\u001b[0m Trial 3 finished with value: 469.8715245355276 and parameters: {'eta': 0.6200609963857777, 'max_depth': 5, 'n_estimators': 89, 'gamma': 0.47096592273509075, 'lambda': 0.4069968808696434, 'alpha': 0.308949313057165}. Best is trial 1 with value: 385.58132285995805.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:01:48,380]\u001b[0m Trial 4 finished with value: 463.75017718031086 and parameters: {'eta': 0.6468771965519099, 'max_depth': 11, 'n_estimators': 89, 'gamma': 0.28672368191584724, 'lambda': 0.22766603591835724, 'alpha': 0.4739153564084242}. Best is trial 1 with value: 385.58132285995805.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:02:29,846]\u001b[0m Trial 5 finished with value: 502.5541114722541 and parameters: {'eta': 0.8419333976743137, 'max_depth': 11, 'n_estimators': 115, 'gamma': 0.98758894173913, 'lambda': 0.40986109442266816, 'alpha': 0.18329844438947973}. Best is trial 1 with value: 385.58132285995805.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:03:11,571]\u001b[0m Trial 6 finished with value: 407.6658642913706 and parameters: {'eta': 0.27836684898872377, 'max_depth': 9, 'n_estimators': 157, 'gamma': 0.8207513429699044, 'lambda': 0.13886642710810426, 'alpha': 0.2633413578162749}. Best is trial 1 with value: 385.58132285995805.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:04:05,712]\u001b[0m Trial 7 finished with value: 410.7581579383196 and parameters: {'eta': 0.19567479315954117, 'max_depth': 11, 'n_estimators': 165, 'gamma': 0.8958439330480388, 'lambda': 0.058865921313795186, 'alpha': 0.395831171368637}. Best is trial 1 with value: 385.58132285995805.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:04:42,717]\u001b[0m Trial 8 finished with value: 490.8807933978972 and parameters: {'eta': 0.790520944651379, 'max_depth': 15, 'n_estimators': 139, 'gamma': 0.9536846441190027, 'lambda': 0.2989929958967444, 'alpha': 0.34772984324113393}. Best is trial 1 with value: 385.58132285995805.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:05:14,107]\u001b[0m Trial 9 finished with value: 466.3973410062788 and parameters: {'eta': 0.517466473015899, 'max_depth': 11, 'n_estimators': 163, 'gamma': 0.5654539130761359, 'lambda': 0.38598569764469914, 'alpha': 0.06790054998940168}. Best is trial 1 with value: 385.58132285995805.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:05:32,141]\u001b[0m Trial 10 finished with value: 384.9483192892662 and parameters: {'eta': 0.040580521732963504, 'max_depth': 6, 'n_estimators': 199, 'gamma': 0.048594749757944866, 'lambda': 0.04312136375875453, 'alpha': 0.4922811422458214}. Best is trial 10 with value: 384.9483192892662.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:05:54,834]\u001b[0m Trial 11 finished with value: 385.0582051987584 and parameters: {'eta': 0.017493962241014533, 'max_depth': 6, 'n_estimators': 198, 'gamma': 0.07478599121005548, 'lambda': 0.014836021657790098, 'alpha': 0.4886339581561613}. Best is trial 10 with value: 384.9483192892662.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:06:26,030]\u001b[0m Trial 12 finished with value: 636.7312395524712 and parameters: {'eta': 0.0028960459451350187, 'max_depth': 6, 'n_estimators': 199, 'gamma': 0.018149360536698328, 'lambda': 0.015459712941053609, 'alpha': 0.4201617451440953}. Best is trial 10 with value: 384.9483192892662.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:07:10,105]\u001b[0m Trial 13 finished with value: 414.9299519152858 and parameters: {'eta': 0.35673372650535845, 'max_depth': 7, 'n_estimators': 199, 'gamma': 0.12134058669853781, 'lambda': 0.14496461732926783, 'alpha': 0.48763074911429183}. Best is trial 10 with value: 384.9483192892662.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:07:47,894]\u001b[0m Trial 14 finished with value: 393.5944023745636 and parameters: {'eta': 0.056142677805974914, 'max_depth': 8, 'n_estimators': 191, 'gamma': 0.294365560100873, 'lambda': 0.12356654156958373, 'alpha': 0.4972412620877727}. Best is trial 10 with value: 384.9483192892662.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:07:59,376]\u001b[0m Trial 15 finished with value: 563.1152497634025 and parameters: {'eta': 0.9825285660180979, 'max_depth': 5, 'n_estimators': 54, 'gamma': 0.0375378113697019, 'lambda': 0.016837374725615, 'alpha': 0.41368225786713375}. Best is trial 10 with value: 384.9483192892662.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:08:46,784]\u001b[0m Trial 16 finished with value: 398.52921916515663 and parameters: {'eta': 0.17295856254563452, 'max_depth': 9, 'n_estimators': 182, 'gamma': 0.33347839460116296, 'lambda': 0.4905662858868652, 'alpha': 0.34890152596207163}. Best is trial 10 with value: 384.9483192892662.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:08:55,659]\u001b[0m Trial 17 finished with value: 407.7717143300032 and parameters: {'eta': 0.33330733955710257, 'max_depth': 2, 'n_estimators': 181, 'gamma': 0.14899491656014807, 'lambda': 0.07109291643684937, 'alpha': 0.4348650718134136}. Best is trial 10 with value: 384.9483192892662.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:09:05,117]\u001b[0m Trial 18 finished with value: 714.0248351147931 and parameters: {'eta': 0.007522688684613322, 'max_depth': 7, 'n_estimators': 55, 'gamma': 0.014722029146575763, 'lambda': 0.19346768363770803, 'alpha': 0.11931623538399816}. Best is trial 10 with value: 384.9483192892662.\u001b[0m\n",
      "\u001b[32m[I 2021-01-13 18:09:27,082]\u001b[0m Trial 19 finished with value: 410.4810406787483 and parameters: {'eta': 0.22760078801112188, 'max_depth': 4, 'n_estimators': 183, 'gamma': 0.2100353705170388, 'lambda': 0.019659661001402803, 'alpha': 0.3580902722823053}. Best is trial 10 with value: 384.9483192892662.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# hyperparam tuning for XGBoost Regressor\n",
    "def xgradboosting_objective(trial):\n",
    "    \n",
    "    eta = trial.suggest_uniform('eta', 0+sys.float_info.min, 1.0)\n",
    "    max_depth = trial.suggest_int('max_depth', min_max_depth, max_max_depth)\n",
    "    n_estimators = trial.suggest_int('n_estimators', min_n_estimators, max_n_estimators)\n",
    "    \n",
    "    gamma = trial.suggest_float('gamma', 0.01, 1.0)\n",
    "    reg_lambda = trial.suggest_uniform('lambda', 0.01, 0.5)\n",
    "    reg_alpha = trial.suggest_uniform('alpha', 0.01, 0.5)\n",
    "\n",
    "    gb_xgb_opt = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                                  booster='gbtree',\n",
    "                                  learning_rate=eta,\n",
    "                                  gamma=gamma,\n",
    "                                  reg_alpha=reg_alpha,\n",
    "                                  reg_lambda=reg_lambda,\n",
    "                                  max_depth=max_depth,\n",
    "                                  n_estimators=n_estimators,\n",
    "                                  random_state=random_state,\n",
    "                                  verbosity=verbose\n",
    "                                 )\n",
    "\n",
    "    gb_xgb_opt = gb_xgb_opt.fit(x_train, y_train)\n",
    "    y_val_pred = gb_xgb_opt.predict(x_val)\n",
    "    \n",
    "    return math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "\n",
    "gb_optuna = optuna.create_study(direction='minimize')\n",
    "start_time = time.time()\n",
    "gb_optuna.optimize(xgradboosting_objective, n_trials=budget)\n",
    "end_time = time.time()\n",
    "\n",
    "summary['gradient_boosting'] = summary['gradient_boosting'].append(pd.Series({\n",
    "    'Time (sec)': '{:.4f}'.format(end_time - start_time), \n",
    "    'Score (RMSE)': gb_optuna.best_value,\n",
    "    'Learning rate': gb_optuna.best_params['eta'],\n",
    "    'Max. depth': gb_optuna.best_params['max_depth'],\n",
    "    'Gamma (min_split_loss)': gb_optuna.best_params['gamma'],\n",
    "    'Lambda': gb_optuna.best_params['lambda'],\n",
    "    'Alpha': gb_optuna.best_params['alpha'],\n",
    "    'N. estimators': gb_optuna.best_params['n_estimators']  \n",
    "    },\n",
    "    name='optuna_xgboost'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Let us take a look at the results. Generally speaking Optuna takes a longer time to execute but in the upside, also obtains better scores.\n",
    "\n",
    "Comparing between the different methods, clearly knn is the worst in terms of scores but time wise, the default is a lot faster. \n",
    "Random Forest and Optuna tuned Scikit Gradient Boosting seem to get similar scores, however, that specific Gradient Boosting implementation takes a very long time to run whereas Random Forest gets one of the best scores in less time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time (sec)</th>\n",
       "      <th>Score (RMSE)</th>\n",
       "      <th>N. neighbors</th>\n",
       "      <th>Weights</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>default</th>\n",
       "      <td>5.0509</td>\n",
       "      <td>455.123868</td>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optuna</th>\n",
       "      <td>117.8632</td>\n",
       "      <td>424.954880</td>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time (sec)  Score (RMSE) N. neighbors   Weights  P\n",
       "default     5.0509    455.123868            5   uniform  2\n",
       "optuna    117.8632    424.954880           11  distance  1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary['knn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time (sec)</th>\n",
       "      <th>Score (RMSE)</th>\n",
       "      <th>Min. samples split</th>\n",
       "      <th>Criterion</th>\n",
       "      <th>Max. depth</th>\n",
       "      <th>N. estimators</th>\n",
       "      <th>Max. features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>default</th>\n",
       "      <td>82.5335</td>\n",
       "      <td>375.560721</td>\n",
       "      <td>2</td>\n",
       "      <td>mse</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optuna</th>\n",
       "      <td>233.6654</td>\n",
       "      <td>374.129312</td>\n",
       "      <td>0.00872</td>\n",
       "      <td>mse</td>\n",
       "      <td>None</td>\n",
       "      <td>171</td>\n",
       "      <td>0.667976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optuna</th>\n",
       "      <td>1031.8257</td>\n",
       "      <td>373.977008</td>\n",
       "      <td>0.00742</td>\n",
       "      <td>mae</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time (sec)  Score (RMSE) Min. samples split Criterion Max. depth  \\\n",
       "default    82.5335    375.560721                  2       mse       None   \n",
       "optuna    233.6654    374.129312            0.00872       mse       None   \n",
       "optuna   1031.8257    373.977008            0.00742       mae        NaN   \n",
       "\n",
       "        N. estimators Max. features  \n",
       "default           100             1  \n",
       "optuna            171      0.667976  \n",
       "optuna            101          sqrt  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary['random_forest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time (sec)</th>\n",
       "      <th>Score (RMSE)</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Gamma (min_split_loss)</th>\n",
       "      <th>Lambda</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Max. depth</th>\n",
       "      <th>N. estimators</th>\n",
       "      <th>Criterion</th>\n",
       "      <th>Max. leaf nodes</th>\n",
       "      <th>Min. samples leaf</th>\n",
       "      <th>Min. samples split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>default</th>\n",
       "      <td>51.8050</td>\n",
       "      <td>389.223359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default_xgboost</th>\n",
       "      <td>12.0105</td>\n",
       "      <td>409.802870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optuna_sklearn</th>\n",
       "      <td>1600.0196</td>\n",
       "      <td>373.503153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.130121</td>\n",
       "      <td>10.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optuna_xgboost</th>\n",
       "      <td>275.7636</td>\n",
       "      <td>384.920573</td>\n",
       "      <td>0.161185</td>\n",
       "      <td>0.745587</td>\n",
       "      <td>0.202009</td>\n",
       "      <td>0.068384</td>\n",
       "      <td>2.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time (sec)  Score (RMSE)     Alpha  Gamma (min_split_loss)  \\\n",
       "default            51.8050    389.223359       NaN                     NaN   \n",
       "default_xgboost    12.0105    409.802870  0.000000                0.000000   \n",
       "optuna_sklearn   1600.0196    373.503153       NaN                     NaN   \n",
       "optuna_xgboost    275.7636    384.920573  0.161185                0.745587   \n",
       "\n",
       "                   Lambda  Learning rate  Max. depth  N. estimators  \\\n",
       "default               NaN       0.100000         3.0          100.0   \n",
       "default_xgboost  1.000000       0.300000         6.0          100.0   \n",
       "optuna_sklearn        NaN       0.130121        10.0           60.0   \n",
       "optuna_xgboost   0.202009       0.068384         2.0          181.0   \n",
       "\n",
       "                    Criterion Max. leaf nodes  Min. samples leaf  \\\n",
       "default          friedman_mse            None                1.0   \n",
       "default_xgboost           NaN               0                NaN   \n",
       "optuna_sklearn   friedman_mse               9                1.0   \n",
       "optuna_xgboost            NaN             NaN                NaN   \n",
       "\n",
       "                 Min. samples split  \n",
       "default                         2.0  \n",
       "default_xgboost                 NaN  \n",
       "optuna_sklearn                  NaN  \n",
       "optuna_xgboost                  NaN  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary['gradient_boosting'].sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666.6691142412727"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dummy regressor(mean)\n",
    "math.sqrt(metrics.mean_squared_error(y_val, [y_val.mean() for i in range(len(y_val))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since between Gradient Boosting and Random Forest there is not a huge difference in scores (taking into account the time it took to run), for the next part of the exercise we will use Random Forest as our base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ATTRIBUTE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Select from all attributes\n",
    "\n",
    "**Are all 550 input attributes actually necessary in order to get a good model? Is it possible to have an accurate model that uses fewer than 550 variables? How many?**\n",
    "\n",
    "For this question we will be using the random forest as in previous sections, but now we will include the parameter for select only certain attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_depth = 2\n",
    "max_max_depth = 25\n",
    "min_n_estimators = 50\n",
    "max_n_estimators = 300\n",
    "min_n_k = 10\n",
    "max_n_k = 550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate whether all 550 attributes are necessary to obtain a good model, we add a new hyper-parameter to the optuna tuning objective function *k*. This hyper-parameter represents the number of attributes needed to obtain a good model. As we want to achieve at least some dimension reduction, we will set the maximum number of attributes to use at 350 of the 550 we had available. (33% reduction)\n",
    "\n",
    "In order to do both feature selection and regression, we create a pipeline that allows us to first select the best k features and once the attributes have been decided upon, apply regression just as we did in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-01-14 16:44:39,594]\u001b[0m A new study created in memory with name: no-name-544ed59b-1d04-40f5-b921-23dcac6c194e\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:44:40,194]\u001b[0m Trial 0 finished with value: 719.6215685321475 and parameters: {'k': 425, 'min_samples_split': 0.9527179423970094, 'criterion': 'mae', 'max_depth': 15, 'n_estimators': 275, 'max_features': 'sqrt'}. Best is trial 0 with value: 719.6215685321475.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:44:40,811]\u001b[0m Trial 1 finished with value: 460.2297015743321 and parameters: {'k': 185, 'min_samples_split': 0.35135192935485027, 'criterion': 'mse', 'max_depth': 4, 'n_estimators': 155, 'max_features': 'log2'}. Best is trial 1 with value: 460.2297015743321.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:44:41,442]\u001b[0m Trial 2 finished with value: 480.29383226267083 and parameters: {'k': 232, 'min_samples_split': 0.5194154418018938, 'criterion': 'mse', 'max_depth': 2, 'n_estimators': 229, 'max_features': 'log2'}. Best is trial 1 with value: 460.2297015743321.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:45:58,969]\u001b[0m Trial 3 finished with value: 417.09772289547345 and parameters: {'k': 393, 'min_samples_split': 0.34377489020775476, 'criterion': 'mae', 'max_depth': 11, 'n_estimators': 284, 'max_features': 'sqrt'}. Best is trial 3 with value: 417.09772289547345.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:45:59,422]\u001b[0m Trial 4 finished with value: 431.76338115122377 and parameters: {'k': 278, 'min_samples_split': 0.23416591106646623, 'criterion': 'mse', 'max_depth': 3, 'n_estimators': 67, 'max_features': 'sqrt'}. Best is trial 3 with value: 417.09772289547345.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:46:21,911]\u001b[0m Trial 5 finished with value: 444.9116938505966 and parameters: {'k': 97, 'min_samples_split': 0.1319909278680338, 'criterion': 'mae', 'max_depth': 6, 'n_estimators': 224, 'max_features': 'log2'}. Best is trial 3 with value: 417.09772289547345.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:47:00,503]\u001b[0m Trial 6 finished with value: 417.637714479501 and parameters: {'k': 390, 'min_samples_split': 0.15039786082174667, 'criterion': 'mae', 'max_depth': 23, 'n_estimators': 237, 'max_features': 'log2'}. Best is trial 3 with value: 417.09772289547345.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:48:06,577]\u001b[0m Trial 7 finished with value: 426.9370717658121 and parameters: {'k': 392, 'min_samples_split': 0.32620248340619307, 'criterion': 'mae', 'max_depth': 3, 'n_estimators': 300, 'max_features': 'sqrt'}. Best is trial 3 with value: 417.09772289547345.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:48:35,864]\u001b[0m Trial 8 finished with value: 523.038181474036 and parameters: {'k': 41, 'min_samples_split': 0.2750300358748493, 'criterion': 'mae', 'max_depth': 11, 'n_estimators': 287, 'max_features': 'sqrt'}. Best is trial 3 with value: 417.09772289547345.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:48:51,449]\u001b[0m Trial 9 finished with value: 467.37337362101994 and parameters: {'k': 313, 'min_samples_split': 0.5120931018507168, 'criterion': 'mae', 'max_depth': 7, 'n_estimators': 100, 'max_features': 'sqrt'}. Best is trial 3 with value: 417.09772289547345.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:48:51,889]\u001b[0m Trial 10 finished with value: 719.5240572053815 and parameters: {'k': 520, 'min_samples_split': 0.7687799829847566, 'criterion': 'mae', 'max_depth': 25, 'n_estimators': 167, 'max_features': 'sqrt'}. Best is trial 3 with value: 417.09772289547345.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:49:34,477]\u001b[0m Trial 11 finished with value: 377.3192898410023 and parameters: {'k': 494, 'min_samples_split': 0.01309566043966992, 'criterion': 'mae', 'max_depth': 25, 'n_estimators': 234, 'max_features': 'log2'}. Best is trial 11 with value: 377.3192898410023.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:50:20,535]\u001b[0m Trial 12 finished with value: 385.33759504782245 and parameters: {'k': 515, 'min_samples_split': 0.05163198801162605, 'criterion': 'mae', 'max_depth': 15, 'n_estimators': 252, 'max_features': 'log2'}. Best is trial 11 with value: 377.3192898410023.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:51:00,649]\u001b[0m Trial 13 finished with value: 377.76855807995884 and parameters: {'k': 535, 'min_samples_split': 0.012345341451012404, 'criterion': 'mae', 'max_depth': 17, 'n_estimators': 199, 'max_features': 'log2'}. Best is trial 11 with value: 377.3192898410023.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:51:41,669]\u001b[0m Trial 14 finished with value: 376.50435848606315 and parameters: {'k': 536, 'min_samples_split': 0.011327545296943489, 'criterion': 'mae', 'max_depth': 24, 'n_estimators': 200, 'max_features': 'log2'}. Best is trial 14 with value: 376.50435848606315.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:52:06,062]\u001b[0m Trial 15 finished with value: 379.5095399536495 and parameters: {'k': 472, 'min_samples_split': 0.020511973522315683, 'criterion': 'mae', 'max_depth': 23, 'n_estimators': 138, 'max_features': 'log2'}. Best is trial 14 with value: 376.50435848606315.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:52:06,535]\u001b[0m Trial 16 finished with value: 719.4250524770866 and parameters: {'k': 550, 'min_samples_split': 0.6740029510665058, 'criterion': 'mae', 'max_depth': 10, 'n_estimators': 197, 'max_features': 'log2'}. Best is trial 14 with value: 376.50435848606315.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:52:07,849]\u001b[0m Trial 17 finished with value: 409.4179043858793 and parameters: {'k': 457, 'min_samples_split': 0.14263341703087007, 'criterion': 'mse', 'max_depth': 25, 'n_estimators': 193, 'max_features': 'log2'}. Best is trial 14 with value: 376.50435848606315.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:52:27,726]\u001b[0m Trial 18 finished with value: 384.0281316075026 and parameters: {'k': 331, 'min_samples_split': 0.04419088489892986, 'criterion': 'mae', 'max_depth': 18, 'n_estimators': 117, 'max_features': 'log2'}. Best is trial 14 with value: 376.50435848606315.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 16:52:28,313]\u001b[0m Trial 19 finished with value: 719.568019638056 and parameters: {'k': 479, 'min_samples_split': 0.9651610019674861, 'criterion': 'mae', 'max_depth': 8, 'n_estimators': 257, 'max_features': 'log2'}. Best is trial 14 with value: 376.50435848606315.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468.71809911727905\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random_state)\n",
    "def random_forest_objective_attr(trial):\n",
    "    k = trial.suggest_int('k', min_n_k, max_n_k)\n",
    "    min_samples_split = trial.suggest_uniform('min_samples_split', 0+sys.float_info.min, 1)\n",
    "    criterion = trial.suggest_categorical('criterion', ['mse','mae'])\n",
    "    max_depth = trial.suggest_int('max_depth', min_max_depth, max_max_depth, log=True)\n",
    "    n_estimators = trial.suggest_int('n_estimators', min_n_estimators, max_n_estimators)\n",
    "    max_features = trial.suggest_uniform('max_features', 0+sys.float_info.min, 0.6)\n",
    "    #max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "\n",
    "    clf = Pipeline([\n",
    "      ('feature_selection', feature_selection.SelectKBest(feature_selection.f_regression, k=k)),\n",
    "      ('regression', ensemble.RandomForestRegressor(\n",
    "          random_state=random_state,\n",
    "          min_samples_split=min_samples_split,\n",
    "          criterion=criterion,\n",
    "          max_depth=max_depth,\n",
    "          n_estimators=n_estimators,\n",
    "          max_features=max_features\n",
    "      ))\n",
    "    ])\n",
    "\n",
    "    clf = clf.fit(x_train, y_train)\n",
    "    y_val_pred = clf.predict(x_val)\n",
    "    return math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "rf_attr_optuna = optuna.create_study(direction='minimize')\n",
    "start_time = time.time()\n",
    "rf_attr_optuna.optimize(random_forest_objective_attr, n_trials=budget, n_jobs=n_jobs)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 536, 'min_samples_split': 0.011327545296943489, 'criterion': 'mae', 'max_depth': 24, 'n_estimators': 200, 'max_features': 'log2'} 376.50435848606315\n"
     ]
    }
   ],
   "source": [
    "print(rf_attr_optuna.best_params, rf_attr_optuna.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the results in terms of the RMSE are similar to the ones we get in the first section with random forest and gradient boosting, but using less amount of attributes. The RMSE is higher, but close to the previous achieved. This means that we were using redundant information to train our models and we can use simpler models with the improvement in trainning time and optimization this means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the performance we will get forecasting energy in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_attr = Pipeline([\n",
    "      ('feature_selection', feature_selection.SelectKBest(feature_selection.f_regression,\n",
    "                                                          k=rf_attr_optuna.best_params['k'])),\n",
    "      ('regression', ensemble.RandomForestRegressor(\n",
    "          random_state=random_state,\n",
    "          min_samples_split=rf_attr_optuna.best_params['min_samples_split'],\n",
    "          criterion=rf_attr_optuna.best_params['criterion'],\n",
    "          max_depth=rf_attr_optuna.best_params['max_depth'],\n",
    "          n_estimators=rf_attr_optuna.best_params['n_estimators'],\n",
    "          max_features=rf_attr_optuna.best_params['max_features']\n",
    "      ))\n",
    "    ])\n",
    "rf_attr = rf_attr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_attr_predict = rf_attr.predict(x_test)\n",
    "print('\\nAttribute selection model performance:')\n",
    "print('RMSE: {:.4f} (should be lower than the trivial predictor using the mean MSE: {:.4f})'.format(\n",
    "    math.sqrt(metrics.mean_squared_error(y_test, rf_attr_predict)),\n",
    "    math.sqrt(metrics.mean_squared_error(y_test, [y_test.mean() for i in range(len(y_test))]))))\n",
    "print('R square: {:.4f} (should be higher than the trivial predictor using the mean: R square {:.4f})'.format(\n",
    "    metrics.r2_score(y_test, rf_attr_predict),\n",
    "    metrics.r2_score(y_test, [y_test.mean() for i in range(len(y_test))])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Use only Sotavento attributes\n",
    "**Is it enough to use only the attributes for the actual Sotavento location? (13th location in the grid)**\n",
    "\n",
    "We will select only Sotavento attributes and use again random forest as in previous section to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2528, 22) (1299, 22) (2110, 22)\n"
     ]
    }
   ],
   "source": [
    "#Selecting sotavento attributes only\n",
    "sot_attr = []\n",
    "for attr in x_train.columns:\n",
    "    if int(attr.split('.')[-1]) == 13:\n",
    "        sot_attr.append(attr)\n",
    "\n",
    "x_train_sot = x_train[sot_attr]\n",
    "x_val_sot = x_val[sot_attr]\n",
    "x_test_sot = x_test[sot_attr]\n",
    "print(x_train_sot.shape,x_val_sot.shape,x_test_sot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-01-14 10:19:39,116]\u001b[0m A new study created in memory with name: no-name-ec0bee82-4b18-4e9d-a9d0-5c72b1145863\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:19:39,551]\u001b[0m Trial 0 finished with value: 668.3650010079921 and parameters: {'min_samples_split': 0.9505332294345612, 'criterion': 'mse', 'max_depth': 6, 'n_estimators': 200, 'max_features': 'sqrt'}. Best is trial 0 with value: 668.3650010079921.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:19:40,195]\u001b[0m Trial 1 finished with value: 719.5115979291787 and parameters: {'min_samples_split': 0.9343306241312633, 'criterion': 'mae', 'max_depth': 4, 'n_estimators': 177, 'max_features': 'auto'}. Best is trial 0 with value: 668.3650010079921.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:19:40,490]\u001b[0m Trial 2 finished with value: 668.3791525575089 and parameters: {'min_samples_split': 0.8438811291334891, 'criterion': 'mse', 'max_depth': 11, 'n_estimators': 168, 'max_features': 'log2'}. Best is trial 0 with value: 668.3650010079921.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:19:40,593]\u001b[0m Trial 3 finished with value: 664.5633825335264 and parameters: {'min_samples_split': 0.6465443868128743, 'criterion': 'mse', 'max_depth': 13, 'n_estimators': 53, 'max_features': 'auto'}. Best is trial 3 with value: 664.5633825335264.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:19:40,836]\u001b[0m Trial 4 finished with value: 668.3866603878097 and parameters: {'min_samples_split': 0.8504389585752644, 'criterion': 'mse', 'max_depth': 7, 'n_estimators': 150, 'max_features': 'log2'}. Best is trial 3 with value: 664.5633825335264.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:19:41,064]\u001b[0m Trial 5 finished with value: 492.7876724160264 and parameters: {'min_samples_split': 0.005219887739545848, 'criterion': 'mse', 'max_depth': 2, 'n_estimators': 64, 'max_features': 'log2'}. Best is trial 5 with value: 492.7876724160264.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:19:41,593]\u001b[0m Trial 6 finished with value: 515.1716637795116 and parameters: {'min_samples_split': 0.5480459356480377, 'criterion': 'mse', 'max_depth': 15, 'n_estimators': 169, 'max_features': 'log2'}. Best is trial 5 with value: 492.7876724160264.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:19:42,700]\u001b[0m Trial 7 finished with value: 391.4466430718656 and parameters: {'min_samples_split': 0.027175149385138875, 'criterion': 'mse', 'max_depth': 13, 'n_estimators': 134, 'max_features': 'log2'}. Best is trial 7 with value: 391.4466430718656.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:19:43,309]\u001b[0m Trial 8 finished with value: 489.06380729522607 and parameters: {'min_samples_split': 0.49958944825875773, 'criterion': 'mse', 'max_depth': 11, 'n_estimators': 183, 'max_features': 'log2'}. Best is trial 7 with value: 391.4466430718656.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:19:43,500]\u001b[0m Trial 9 finished with value: 668.4282067060162 and parameters: {'min_samples_split': 0.9924244786413027, 'criterion': 'mse', 'max_depth': 8, 'n_estimators': 111, 'max_features': 'sqrt'}. Best is trial 7 with value: 391.4466430718656.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:21:20,265]\u001b[0m Trial 10 finished with value: 389.67465393917604 and parameters: {'min_samples_split': 0.010006269278679033, 'criterion': 'mae', 'max_depth': 14, 'n_estimators': 107, 'max_features': 'auto'}. Best is trial 10 with value: 389.67465393917604.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:22:58,166]\u001b[0m Trial 11 finished with value: 396.96336987320836 and parameters: {'min_samples_split': 0.02952447637394945, 'criterion': 'mae', 'max_depth': 15, 'n_estimators': 105, 'max_features': 'auto'}. Best is trial 10 with value: 389.67465393917604.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:24:02,669]\u001b[0m Trial 12 finished with value: 425.8118649571287 and parameters: {'min_samples_split': 0.2168512035668756, 'criterion': 'mae', 'max_depth': 12, 'n_estimators': 88, 'max_features': 'auto'}. Best is trial 10 with value: 389.67465393917604.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:25:50,005]\u001b[0m Trial 13 finished with value: 425.25666000753506 and parameters: {'min_samples_split': 0.20844010918150993, 'criterion': 'mae', 'max_depth': 14, 'n_estimators': 138, 'max_features': 'auto'}. Best is trial 10 with value: 389.67465393917604.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:26:00,699]\u001b[0m Trial 14 finished with value: 436.8205109975083 and parameters: {'min_samples_split': 0.1730178411873519, 'criterion': 'mae', 'max_depth': 10, 'n_estimators': 82, 'max_features': 'sqrt'}. Best is trial 10 with value: 389.67465393917604.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:26:14,236]\u001b[0m Trial 15 finished with value: 455.815371716394 and parameters: {'min_samples_split': 0.3548719408424206, 'criterion': 'mae', 'max_depth': 13, 'n_estimators': 127, 'max_features': 'log2'}. Best is trial 10 with value: 389.67465393917604.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:27:46,477]\u001b[0m Trial 16 finished with value: 412.5472410511345 and parameters: {'min_samples_split': 0.08916366909545262, 'criterion': 'mae', 'max_depth': 15, 'n_estimators': 111, 'max_features': 'auto'}. Best is trial 10 with value: 389.67465393917604.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:29:08,600]\u001b[0m Trial 17 finished with value: 434.2609064552973 and parameters: {'min_samples_split': 0.32738198385098427, 'criterion': 'mae', 'max_depth': 10, 'n_estimators': 149, 'max_features': 'auto'}. Best is trial 10 with value: 389.67465393917604.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:29:09,454]\u001b[0m Trial 18 finished with value: 384.02242946450264 and parameters: {'min_samples_split': 0.002163316212452219, 'criterion': 'mse', 'max_depth': 14, 'n_estimators': 87, 'max_features': 'log2'}. Best is trial 18 with value: 384.02242946450264.\u001b[0m\n",
      "\u001b[32m[I 2021-01-14 10:29:53,570]\u001b[0m Trial 19 finished with value: 436.6303415352574 and parameters: {'min_samples_split': 0.3407328785505106, 'criterion': 'mae', 'max_depth': 9, 'n_estimators': 82, 'max_features': 'auto'}. Best is trial 18 with value: 384.02242946450264.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random_state)\n",
    "def random_forest_sot_objective(trial):\n",
    "    min_samples_split = trial.suggest_uniform('min_samples_split', 0+sys.float_info.min, 1)\n",
    "    criterion = trial.suggest_categorical('criterion', ['mse','mae'])\n",
    "    max_depth = trial.suggest_int('max_depth', min_max_depth, max_max_depth)\n",
    "    n_estimators = trial.suggest_int('n_estimators', min_n_estimators, max_n_estimators)\n",
    "    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2'])\n",
    "\n",
    "    clf = ensemble.RandomForestRegressor(\n",
    "        random_state=random_state,\n",
    "        min_samples_split=min_samples_split,\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        max_features=max_features\n",
    "        )\n",
    "\n",
    "    clf = clf.fit(x_train_sot, y_train)\n",
    "    y_val_pred = clf.predict(x_val_sot)\n",
    "    return math.sqrt(metrics.mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "rf_sot_optuna = optuna.create_study(direction='minimize')\n",
    "start_time = time.time()\n",
    "rf_sot_optuna.optimize(random_forest_sot_objective, n_trials=budget, n_jobs=n_jobs)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 0.002163316212452219, 'criterion': 'mse', 'max_depth': 14, 'n_estimators': 87, 'max_features': 'log2'} 384.02242946450264\n"
     ]
    }
   ],
   "source": [
    "print(rf_sot_optuna.best_params, rf_sot_optuna.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use a model using only the Sotavento attributes we can achive similar results as in the first section. Again, the RMSE is not as low as we have already obtained, but now we are using 13 attributes instead of the 550 availables so the tradeof between performance and execution time does really pay off.\n",
    "\n",
    "We will now estimate the performance in predicting energy generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_sot = Pipeline([\n",
    "      ('feature_selection', feature_selection.SelectKBest(feature_selection.f_regression,\n",
    "                                                          k=rf_sot_optuna.best_params['k'])),\n",
    "      ('regression', ensemble.RandomForestRegressor(\n",
    "          random_state=random_state,\n",
    "          min_samples_split=rf_sot_optuna.best_params['min_samples_split'],\n",
    "          criterion=rf_sot_optuna.best_params['criterion'],\n",
    "          max_depth=rf_sot_optuna.best_params['max_depth'],\n",
    "          n_estimators=rf_sot_optuna.best_params['n_estimators'],\n",
    "          max_features=rf_sot_optuna.best_params['max_features']\n",
    "      ))\n",
    "    ])\n",
    "rf_sot = rf_attr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_sot_predict = rf_sot.predict(x_test)\n",
    "print('\\nAttribute selection model performance:')\n",
    "print('RMSE: {:.4f} (should be lower than the trivial predictor using the mean MSE: {:.4f})'.format(\n",
    "    math.sqrt(metrics.mean_squared_error(y_test, rf_sot_predict)),\n",
    "    math.sqrt(metrics.mean_squared_error(y_test, [y_test.mean() for i in range(len(y_test))]))))\n",
    "print('R square: {:.4f} (should be higher than the trivial predictor using the mean: R square {:.4f})'.format(\n",
    "    metrics.r2_score(y_test, rf_sot_predict),\n",
    "    metrics.r2_score(y_test, [y_test.mean() for i in range(len(y_test))])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusions\n",
    "\n",
    "We have obtained really good models to forecast energy generation with precision even though we artificially remove data from the dataset. First, we had to deal with this missing data. As the matrix we are dealing with is really big and due to our computing power restrictions, we could not really use advanced techniques to imput missing data as an iterative imputer or a KNN imputer. Therefore, we just put the sample median in the empty spots we generated.\n",
    "\n",
    "Then, we have trained KNN, random forest and gradient boosting models. We have used an advanced hyper-parameter tunning optimizer as optuna and also xgboost library to get a better approach than with sklearn which models predict worst and are slower for trainning in this case. Again, we have to deal with the problem of computation time and because of computing power restirctions we cannot set a really high budget or times for trainning will be  really high for the assignment. After all, we obtained two good models for random forest and gradient boosting (xgboost) that could be used to predict energy generation.\n",
    "\n",
    "Lastly, we have tried to do some attribute selection. We first try it by using pure computation and with Pipelines to first select the best attributes to predict and then train a random forest model with these specifications. The results were slightly worse than trainning with all the attributes, but satisfactory taking into account the dimension reduction. Then, we use only the Sotavento data that we have and again get an interesting model, not as good as the one using all the parameters, but another time sufficient if we think in the dimension reduction.\n",
    "\n",
    "As a final conclusion, this assignment has made us undertand the importance of planning when trainning really big models as a lot of time could be wasted. We get the impression that we could have achieved quite better results with more powerful computers that will have helped us with deeper hyper-parameter tunning, but we are satisfied with the results, the knowledge and conclusion we have obtained during the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
