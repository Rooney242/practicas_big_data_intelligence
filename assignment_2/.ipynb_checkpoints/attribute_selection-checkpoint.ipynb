{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTRIBUTE / FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, relevant libraries are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Attribute selection methods from sklearn\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, chi2, f_classif, f_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANKING/FILTER ATTRIBUTE SELECTION WITH TRAIN / TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "print(X.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the estimated root mean squared error (RMSE) with the original dataset (all the attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeRegressor(random_state=0)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print metrics.mean_squared_error(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "# We want to rank all attributes, and the best ones will be selected later\n",
    "selector = SelectKBest(f_regression, k=\"all\")\n",
    "selector.fit(X_train, y_train)\n",
    "sorted_attributes = np.argsort(-selector.scores_)\n",
    "sorted_scores = np.sort(-selector.scores_)\n",
    "for index,element in enumerate(zip(sorted_attributes, sorted_scores)):\n",
    "    print element\n",
    "    if index>10: break\n",
    "        \n",
    "plt.plot(-sorted_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the first ten attributes have the highest scores. Let's see what happens if we select only those 9 attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before doing that, in order to get a meaningful comparison, we are going to re-order the attributes in the dataset, because it seems that decision trees give different results if the attributes appear in a different order in the dataset (even if the attributes are exactly the same!). So, first we obtain again the test results again with attributes in the original order. It can be seen that we get the same result as before (it is important to set the random state to the same value as before: 0. Probably we would get a different result by setting the random state to a different value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeRegressor(random_state=0)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print metrics.mean_squared_error(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we re-order the attributes according to their relevance, and compute the test error again. Please, notice that the attributes are exactly the same, just in a different order! Even so, we get a different test result because the order of attributes is taken into account somehow by the decision tree sklearn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_new = X_train[:, sorted_attributes]\n",
    "X_test_new = X_test[:, sorted_attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeRegressor(random_state=0)\n",
    "clf = clf.fit(X_train_new, y_train)\n",
    "y_test_pred = clf.predict(X_test_new)\n",
    "print metrics.mean_squared_error(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see what happens if we use only the 10 most relevant attributes. We can see that the error is not too different, even though we have removed three attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeRegressor(random_state=0)\n",
    "clf = clf.fit(X_train_new[:,:10], y_train)\n",
    "y_test_pred = clf.predict(X_test_new[:,:10])\n",
    "print metrics.mean_squared_error(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did before was to check that we can get approximately the same test error using fewer attributes. But we know that we cannot use the test set in order to select the optimal number of features, because we can only use the training set for that (or for tuning any other hyper-parameter, for that matter). We are going to do attribute selection properly by considering the number of attributes as a hyper-parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do that, we are going to construct a method which is a combination or a sequence (a pipeline, in fact) of an attribute selector + a decision tree regressor. clf is therefore the pipeline (a sequence of attribute selection + regression algorithm). The number of attributes to be selected is a hyper-parameter of clf. max_depth is also a hyper-parameter of clf. We can use grid search in order to tune both parameters at the same time. Please notice that we do the tuning using only the training set. At the end of GridSearch, clf_grid contains a model trained with the whole training set and the optimal hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = {'feature_selection__k': np.arange(X_train.shape[1])+1,\n",
    "             'regression__min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14]}\n",
    "\n",
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectKBest(f_regression)),\n",
    "  ('regression', tree.DecisionTreeRegressor(random_state=0))\n",
    "])\n",
    "\n",
    "#n_jobs is for parallelizing gridsearch. It might be the case that it does\n",
    "# not work well in Windows, hence n_jobs=1\n",
    "np.random.seed(0)\n",
    "clf_grid = GridSearchCV(clf, \n",
    "                        param_grid,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=5 , n_jobs=1, verbose=1)\n",
    "%time _ = clf_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the best hyper-parameters. It seems that for this case, all the attributes should (13) should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print clf_grid.best_params_, clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we test the tuned pipeline (clf_grid = attribute selection+tree) with the test partition. Please note, that at prediction time, clf_grid will first select the same 13 attributes selected in training, and then it will apply the decision tree trained on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf_grid.predict(X_test)\n",
    "print metrics.mean_squared_error(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING PCA FOR TRANSFORMING ATTRIBUTES WITH TRAIN / TEST EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print X.shape\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the maximum number of PCA components for the moment(4 iris attributes implies 4 PCA components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=4)\n",
    "pca.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how much variance explains each of the four components. We can see that the first component explaines most of the variance 92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the cummulative variance explained by n components. It seems that with 2 components we can already explain more than 95% of the variance. Using that criterion, we should use 2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(accumulated_variance)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, let's compute 2 PCA components and apply them to train and test. We can see that the new input attributes (X_train_new and X_test_new have 2 new attributes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(X_train)\n",
    "X_train_new = pca.transform(X_train)\n",
    "X_test_new = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can apply a classifier to the new, reduced, training set, and test it on the transformed test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "clf = clf.fit(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = clf.predict(X_train_new)\n",
    "y_test_pred = clf.predict(X_test_new)\n",
    "print metrics.accuracy_score(y_train, y_train_pred)\n",
    "print metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we construct the tree with the original dataset (before applying PCA), we see that test accuracy is larger with 4 attributes than with 2 PCA components. So in this case, PCA would not be useful from an accuracy point of view, but it would be useful to reduce the complexity of the model (with PCA we have only 2 components instead of the 4 original attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print metrics.accuracy_score(y_train, y_train_pred)\n",
    "print metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also consider that the number of PCA components to be selected is a hyper-parameter, and use a similar pipeline to the one at the first part of the tutorial, in order to select the right number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
